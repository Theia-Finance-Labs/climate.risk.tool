{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2479d82-0956-459c-b4c4-6d1088b222cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Compound_FWI_ensemble_return_periods.nc ...\n",
      "✅ Saved: Compound_FWI_ensemble_return_periods.csv\n",
      "Processing Compound_HI_ensemble_return_period.nc ...\n",
      "✅ Saved: Compound_HI_ensemble_return_period.csv\n",
      "Processing Drought_SPI6_ensemble_return_period.nc ...\n",
      "✅ Saved: Drought_SPI6_ensemble_return_period.csv\n",
      "\n",
      "✅ All NetCDFs converted to CSVs.\n",
      "CSV files saved in:\n",
      "/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Climate Maps CSVs\n"
     ]
    }
   ],
   "source": [
    "# --- SCRIPT 1: Convert all .nc hazard maps into CSVs ---\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# === PATHS ===\n",
    "example_nc = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Relevant Climate Hazard Maps/Compound_FWI_ensemble_return_periods.nc\"\n",
    "nc_dir = os.path.dirname(example_nc)\n",
    "\n",
    "csv_out_dir = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Climate Maps CSVs\"\n",
    "os.makedirs(csv_out_dir, exist_ok=True)\n",
    "\n",
    "# === FILENAME PATTERN PARSER ===\n",
    "FNAME_RE = re.compile(r\"([^/_]+)_([^/_]+)_ensemble_return_period\", flags=re.IGNORECASE)\n",
    "\n",
    "def parse_hazard_from_fname(path):\n",
    "    base = os.path.basename(path)\n",
    "    m = FNAME_RE.search(base)\n",
    "    if not m:\n",
    "        return (\"unknown\", \"unknown\")\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "def nc_to_csv(nc_path, out_dir):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    var_name = list(ds.data_vars.keys())[0]\n",
    "    da = ds[var_name]\n",
    "    \n",
    "    # Expecting dims: ensemble, GWL, return_period, lat, lon\n",
    "    df = da.to_dataframe(name=\"value\").reset_index().dropna(subset=[\"value\"])\n",
    "    \n",
    "    # Add hazard info from filename\n",
    "    h_type, h_ind = parse_hazard_from_fname(nc_path)\n",
    "    df[\"hazard_type\"] = h_type\n",
    "    df[\"hazard_indicator\"] = h_ind\n",
    "    \n",
    "    # Save to CSV\n",
    "    out_name = os.path.splitext(os.path.basename(nc_path))[0] + \".csv\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"✅ Saved: {out_name}\")\n",
    "    return out_path\n",
    "\n",
    "# === RUN ===\n",
    "nc_files = sorted(glob.glob(os.path.join(nc_dir, \"*_ensemble_return_period*.nc\")))\n",
    "if not nc_files:\n",
    "    raise FileNotFoundError(f\"No .nc files found in {nc_dir}\")\n",
    "\n",
    "for fp in nc_files:\n",
    "    print(f\"Processing {os.path.basename(fp)} ...\")\n",
    "    nc_to_csv(fp, csv_out_dir)\n",
    "\n",
    "print(\"\\n✅ All NetCDFs converted to CSVs.\")\n",
    "print(f\"CSV files saved in:\\n{csv_out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94e8376-0567-4005-a396-7571bbf4e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: Compound_FWI_ensemble_return_periods.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/y_l0wg597cv20ss22dd13kph0000gn/T/ipykernel_1644/148396.py:101: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: Compound_HI_ensemble_return_period.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/y_l0wg597cv20ss22dd13kph0000gn/T/ipykernel_1644/148396.py:101: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: Drought_SPI6_ensemble_return_period.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/y_l0wg597cv20ss22dd13kph0000gn/T/ipykernel_1644/148396.py:101: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Aggregation complete! Encoding repaired with ftfy.\n",
      "Saved to:\n",
      "/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_hazards.csv\n",
      "     region  adm_level scenario_code scenario_name  hazard_return_period  \\\n",
      "0      Acre          1           1.5           1.5                     5   \n",
      "1   Alagoas          1           1.5           1.5                     5   \n",
      "2     Amapa          1           1.5           1.5                     5   \n",
      "3  Amazonas          1           1.5           1.5                     5   \n",
      "4     Bahia          1           1.5           1.5                     5   \n",
      "\n",
      "  hazard_type hazard_indicator        min          max       mean     median  \\\n",
      "0    Compound              FWI  14.124794    20.445835  17.205294  17.109316   \n",
      "1    Compound              FWI  20.024550    34.537330  25.723636  24.414597   \n",
      "2    Compound              FWI   9.447371  9724.408000  79.248007  12.288034   \n",
      "3    Compound              FWI   5.976500    39.535260  13.456912  13.577146   \n",
      "4    Compound              FWI  15.821462    75.393005  36.566402  39.410175   \n",
      "\n",
      "        p2_5         p5        p10        p90        p95      p97_5  \n",
      "0  14.381408  14.636077  15.063038  19.448585  19.606652  19.899502  \n",
      "1  20.048070  20.099225  20.361241  32.236353  32.721556  33.066912  \n",
      "2   9.643834   9.753501  10.092148  15.305959  16.488098  36.846513  \n",
      "3   6.797001   7.448665   8.312537  18.200407  18.705656  19.243897  \n",
      "4  17.712848  19.043615  21.467598  45.941574  46.986167  48.583872  \n"
     ]
    }
   ],
   "source": [
    "# --- SCRIPT 2: Aggregate CSVs over ADM1 and ADM2 ---\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# === PATHS ===\n",
    "csv_dir = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Climate Maps CSVs\"\n",
    "adm1_path = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Tool/Brazil Borders/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp\"\n",
    "adm2_path = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Tool/Brazil Borders/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp\"\n",
    "output_dir = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"precomputed_adm_nc.csv\")\n",
    "\n",
    "# === LOAD ADM SHAPEFILES ===\n",
    "def load_adm(adm_path):\n",
    "    gdf = gpd.read_file(adm_path).to_crs(\"EPSG:4326\")\n",
    "    for c in [\"shapeName\", \"NAME_2\", \"NAME_1\", \"NAME\", \"prov_name\"]:\n",
    "        if c in gdf.columns:\n",
    "            return gdf, c\n",
    "    gdf[\"region_name\"] = gdf.index.astype(str)\n",
    "    return gdf, \"region_name\"\n",
    "\n",
    "adm1, adm1_name = load_adm(adm1_path)\n",
    "adm2, adm2_name = load_adm(adm2_path)\n",
    "\n",
    "# === AGGREGATION FUNCTION ===\n",
    "def summarize_points_over_adm(points_df, adm_gdf, adm_name_col, adm_level):\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        points_df,\n",
    "        geometry=gpd.points_from_xy(points_df[\"lon\"], points_df[\"lat\"]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    joined = gpd.sjoin(gdf_pts, adm_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "    def q(p): return lambda x: float(np.nanpercentile(x, p))\n",
    "\n",
    "    agg = (\n",
    "        joined.groupby(\n",
    "            [\"GWL\", \"return_period\", \"ensemble\", \"hazard_type\", \"hazard_indicator\", adm_name_col],\n",
    "            dropna=False,\n",
    "        )\n",
    "        .agg(\n",
    "            min=(\"value\", \"min\"),\n",
    "            max=(\"value\", \"max\"),\n",
    "            mean=(\"value\", \"mean\"),\n",
    "            median=(\"value\", \"median\"),\n",
    "            p2_5=(\"value\", q(2.5)),\n",
    "            p5=(\"value\", q(5)),\n",
    "            p10=(\"value\", q(10)),\n",
    "            p90=(\"value\", q(90)),\n",
    "            p95=(\"value\", q(95)),\n",
    "            p97_5=(\"value\", q(97.5)),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    agg = agg.rename(\n",
    "        columns={\n",
    "            \"GWL\": \"scenario_code\",\n",
    "            \"return_period\": \"hazard_return_period\",\n",
    "            adm_name_col: \"region\",\n",
    "        }\n",
    "    )\n",
    "    agg[\"scenario_name\"] = agg[\"scenario_code\"]\n",
    "    agg[\"adm_level\"] = adm_level\n",
    "\n",
    "    cols = [\n",
    "        \"region\",\n",
    "        \"adm_level\",\n",
    "        \"scenario_code\",\n",
    "        \"scenario_name\",\n",
    "        \"hazard_return_period\",\n",
    "        \"hazard_type\",\n",
    "        \"hazard_indicator\",\n",
    "        \"min\",\n",
    "        \"max\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"p2_5\",\n",
    "        \"p5\",\n",
    "        \"p10\",\n",
    "        \"p90\",\n",
    "        \"p95\",\n",
    "        \"p97_5\",\n",
    "    ]\n",
    "    return agg[cols]\n",
    "\n",
    "# === RUN AGGREGATION ===\n",
    "csv_files = sorted(glob.glob(os.path.join(csv_dir, \"*.csv\")))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSVs found in {csv_dir}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for csv_fp in csv_files:\n",
    "    print(f\"Aggregating: {os.path.basename(csv_fp)}\")\n",
    "    df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n",
    "\n",
    "    if \"value\" not in df.columns:\n",
    "        possible_val = [c for c in df.columns if c not in [\"GWL\", \"lon\", \"lat\", \"return_period\", \"ensemble\", \"hazard_type\", \"hazard_indicator\"]]\n",
    "        if possible_val:\n",
    "            df = df.rename(columns={possible_val[0]: \"value\"})\n",
    "\n",
    "    res_adm1 = summarize_points_over_adm(df, adm1, adm1_name, adm_level=1)\n",
    "    res_adm2 = summarize_points_over_adm(df, adm2, adm2_name, adm_level=2)\n",
    "\n",
    "    all_results.extend([res_adm1, res_adm2])\n",
    "\n",
    "# === COMBINE AND REPAIR ENCODING ===\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# --- FIX MOJIBAKE (e.g. √Ågua → Água) ---\n",
    "\n",
    "# try several encodings automatically\n",
    "import ftfy  # Fixes Text For You — robust accent repair\n",
    "\n",
    "def fix_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return ftfy.fix_text(str(text))\n",
    "\n",
    "# Apply only to region names\n",
    "final_df[\"region\"] = final_df[\"region\"].apply(fix_text)\n",
    "\n",
    "# --- ENSURE UTF-8 OUTPUT ---\n",
    "output_path = os.path.join(output_dir, \"precomputed_adm_nc.csv\")\n",
    "final_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n✅ Aggregation complete! Encoding repaired with ftfy.\")\n",
    "print(f\"Saved to:\\n{output_path}\")\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afbe086c-7b78-4a93-bc17-3f58dfb03a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m geom \u001b[38;5;241m=\u001b[39m [row\u001b[38;5;241m.\u001b[39mgeometry]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     out_image, out_transform \u001b[38;5;241m=\u001b[39m rasterio\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mmask(src, geom, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madm_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msc_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, RP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/rasterio/mask.py:197\u001b[0m, in \u001b[0;36mmask\u001b[0;34m(dataset, shapes, all_touched, invert, nodata, filled, crop, pad, pad_width, indexes)\u001b[0m\n\u001b[1;32m    194\u001b[0m out_image\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m out_image\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m|\u001b[39m shape_mask\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filled:\n\u001b[0;32m--> 197\u001b[0m     out_image \u001b[38;5;241m=\u001b[39m out_image\u001b[38;5;241m.\u001b[39mfilled(nodata)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_image, transform\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:3851\u001b[0m, in \u001b[0;36mMaskedArray.filled\u001b[0;34m(self, fill_value)\u001b[0m\n\u001b[1;32m   3849\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3850\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3851\u001b[0m     np\u001b[38;5;241m.\u001b[39mcopyto(result, fill_value, where\u001b[38;5;241m=\u001b[39mm)\n\u001b[1;32m   3852\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m   3853\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m narray(fill_value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### add flood hazard\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "\n",
    "# --- INPUTS & OUTPUTS ---\n",
    "\n",
    "# ADM boundaries\n",
    "adm_levels = [\n",
    "    (\"ADM1\", \"/Users/2diigermany/Desktop/Personal/Brazil/Content/Physical Risk Data/QGIS/ Boundaries/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp\"),\n",
    "    (\"ADM2\", \"/Users/2diigermany/Desktop/Personal/Brazil/Content/Physical Risk Data/QGIS/ Boundaries/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp\"),\n",
    "]\n",
    "\n",
    "# Flood maps directory\n",
    "flood_maps_dir = \"/Users/2diigermany/Desktop/Work/GRI Flood Maps/Flood Maps/Brazil relevant Flood Maps\"\n",
    "\n",
    "# Scenarios and return periods\n",
    "scenario_codes = [\"pc\", \"rcp26\", \"rcp85\"]\n",
    "scenario_labels = {\"pc\": \"CurrentClimate\", \"rcp26\": \"RCP2.6\", \"rcp85\": \"RCP8.5\"}\n",
    "scenario_code_map_for_output = {\"pc\": \"present\", \"rcp26\": \"rcp26\", \"rcp85\": \"rcp85\"}\n",
    "return_periods = [5, 10, 25, 50, 100, 1000]\n",
    "\n",
    "# Output CSV\n",
    "output_csv = \"'/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_flood.csv'\"\n",
    "\n",
    "# --- HELPERS ---\n",
    "\n",
    "def fix_text(s):\n",
    "    \"\"\"Try to repair mojibake / encoding artefacts; safe no-op if already fine.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    try:\n",
    "        return s.encode(\"cp1252\").decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def load_adm(adm_path):\n",
    "    \"\"\"Load a shapefile and pick a reasonable region-name column.\"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(adm_path)\n",
    "    except UnicodeDecodeError:\n",
    "        gdf = gpd.read_file(adm_path, encoding=\"latin1\")\n",
    "    # choose name column\n",
    "    name_col = None\n",
    "    for c in [\"shapeName\", \"NAME_2\", \"NAME_1\", \"NAME\", \"name\", \"prov_name\"]:\n",
    "        if c in gdf.columns:\n",
    "            name_col = c\n",
    "            break\n",
    "    if name_col is None:\n",
    "        gdf[\"region\"] = gdf.index.astype(str)\n",
    "    else:\n",
    "        gdf[\"region\"] = gdf[name_col]\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(fix_text)\n",
    "    return gdf\n",
    "\n",
    "def percentile(arr, q):\n",
    "    return float(np.percentile(arr, q)) if arr.size else np.nan\n",
    "\n",
    "# --- PROCESSING ---\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    gdf = load_adm(adm_path)\n",
    "\n",
    "    for sc in scenario_codes:\n",
    "        sc_label = scenario_labels[sc]\n",
    "        sc_code_out = scenario_code_map_for_output[sc]\n",
    "\n",
    "        for rp in return_periods:\n",
    "            tif_name = f\"global_{sc}_h{rp}glob.tif\"\n",
    "            tif_path = os.path.join(flood_maps_dir, tif_name)\n",
    "\n",
    "            # Only process existing .tif files\n",
    "            if not (tif_path.endswith(\".tif\") and os.path.exists(tif_path)):\n",
    "                print(f\"Skipping (not found): {tif_path}\")\n",
    "                continue\n",
    "\n",
    "            with rasterio.open(tif_path) as src:\n",
    "                nodata = src.nodata\n",
    "\n",
    "                # Reproject ADM to raster CRS if needed\n",
    "                if gdf.crs is None:\n",
    "                    gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                if gdf.crs != src.crs:\n",
    "                    gdf_proj = gdf.to_crs(src.crs)\n",
    "                else:\n",
    "                    gdf_proj = gdf\n",
    "\n",
    "                for idx, row in gdf_proj.iterrows():\n",
    "                    geom = [row.geometry]\n",
    "                    try:\n",
    "                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Mask error ({adm_label}, {row['region']}, {sc_label}, RP {rp}): {e}\")\n",
    "                        continue\n",
    "\n",
    "                    data = out_image[0]\n",
    "                    if nodata is not None:\n",
    "                        valid_mask = data != nodata\n",
    "                    else:\n",
    "                        valid_mask = np.ones(data.shape, dtype=bool)\n",
    "\n",
    "                    valid = data[valid_mask]\n",
    "                    # If masked array, compress\n",
    "                    if np.ma.isMaskedArray(valid):\n",
    "                        valid = valid.compressed()\n",
    "\n",
    "                    # Compute stats\n",
    "                    if valid.size == 0:\n",
    "                        stats = dict(\n",
    "                            min=np.nan, max=np.nan, mean=np.nan, median=np.nan,\n",
    "                            p2_5=np.nan, p5=np.nan, p10=np.nan, p90=np.nan, p95=np.nan, p97_5=np.nan\n",
    "                        )\n",
    "                    else:\n",
    "                        stats = dict(\n",
    "                            min=float(np.min(valid)),\n",
    "                            max=float(np.max(valid)),\n",
    "                            mean=float(np.mean(valid)),\n",
    "                            median=percentile(valid, 50),\n",
    "                            p2_5=percentile(valid, 2.5),\n",
    "                            p5=percentile(valid, 5),\n",
    "                            p10=percentile(valid, 10),\n",
    "                            p90=percentile(valid, 90),\n",
    "                            p95=percentile(valid, 95),\n",
    "                            p97_5=percentile(valid, 97.5),\n",
    "                        )\n",
    "\n",
    "                    # Build output row (match precomputed_adm_hazards.csv format)\n",
    "                    out_row = {\n",
    "                        \"region\": fix_text(gdf.loc[idx, \"region\"]),   # original name in original CRS GeoDF\n",
    "                        \"adm_level\": 1 if adm_label == \"ADM1\" else 2,\n",
    "                        \"scenario_code\": sc_code_out,                 # 'present' | 'rcp26' | 'rcp85'\n",
    "                        \"scenario_name\": sc_label,                    # 'CurrentClimate' | 'RCP2.6' | 'RCP8.5'\n",
    "                        \"hazard_return_period\": rp,\n",
    "                        \"hazard_type\": \"FloodTIF\",\n",
    "                        \"hazard_indicator\": \"Flood Height\",\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"median\": stats[\"median\"],\n",
    "                        \"p2_5\": stats[\"p2_5\"],\n",
    "                        \"p5\": stats[\"p5\"],\n",
    "                        \"p10\": stats[\"p10\"],\n",
    "                        \"p90\": stats[\"p90\"],\n",
    "                        \"p95\": stats[\"p95\"],\n",
    "                        \"p97_5\": stats[\"p97_5\"],\n",
    "                    }\n",
    "                    all_rows.append(out_row)\n",
    "\n",
    "# --- EXPORT ---\n",
    "\n",
    "df_out = pd.DataFrame(all_rows, columns=[\n",
    "    \"region\",\"adm_level\",\"scenario_code\",\"scenario_name\",\"hazard_return_period\",\n",
    "    \"hazard_type\",\"hazard_indicator\",\"min\",\"max\",\"mean\",\"median\",\"p2_5\",\"p5\",\"p10\",\"p90\",\"p95\",\"p97_5\"\n",
    "])\n",
    "\n",
    "# Ensure accent-safe output for Excel (UTF-8 with BOM)\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_out.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Saved flood precompute to:\\n{output_csv}\")\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cc5d4d6-d219-4b77-9238-3315c6561c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File saved successfully:\n",
      "/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_flood1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ✅ Corrected output path (no extra quotes)\n",
    "output_csv = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_flood1.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "\n",
    "# Save again with proper encoding to preserve accents (UTF-8 BOM)\n",
    "df_out.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ File saved successfully:\\n{output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f5f3b4-7c32-485c-9cc2-6f5d405e3469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/y_l0wg597cv20ss22dd13kph0000gn/T/ipykernel_1644/1369163731.py:11: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_nc = pd.read_csv(nc_file, encoding=\"utf-8-sig\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset saved successfully to:\n",
      "/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_hazards.csv\n",
      "Total records: 865225\n",
      "Columns: ['region', 'adm_level', 'scenario_code', 'scenario_name', 'hazard_return_period', 'hazard_type', 'hazard_indicator', 'min', 'max', 'mean', 'median', 'p2_5', 'p5', 'p10', 'p90', 'p95', 'p97_5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === INPUT FILES ===\n",
    "nc_file = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_nc.csv\"\n",
    "flood_file = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_flood.csv\"\n",
    "\n",
    "# === OUTPUT FILE ===\n",
    "output_file = \"/Users/2diigermany/2° Investing Dropbox/Antonio Buller/PortCheck_v2/10_Projects/GIZ Brazil/Climate Work/Hazard Maps/Climate Data/Precomputated Regional Data/precomputed_adm_hazards.csv\"\n",
    "\n",
    "# === LOAD FILES (UTF-8 SAFE) ===\n",
    "df_nc = pd.read_csv(nc_file, encoding=\"utf-8-sig\")\n",
    "df_flood = pd.read_csv(flood_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# === ALIGN COLUMNS ===\n",
    "# Get the union of all columns\n",
    "all_cols = sorted(set(df_nc.columns).union(set(df_flood.columns)))\n",
    "\n",
    "# Add missing columns as NaN so both match perfectly\n",
    "for df in [df_nc, df_flood]:\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    df = df[all_cols]\n",
    "\n",
    "# === MERGE (STACK) ===\n",
    "df_merged = pd.concat([df_nc, df_flood], ignore_index=True)\n",
    "\n",
    "# === SAVE OUTPUT ===\n",
    "df_merged.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Merged dataset saved successfully to:\\n{output_file}\")\n",
    "print(f\"Total records: {len(df_merged)}\")\n",
    "print(\"Columns:\", list(df_merged.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a887b-ce10-4ee4-bd9f-73843bb869d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
