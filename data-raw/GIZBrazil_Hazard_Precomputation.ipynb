{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2479d82-0956-459c-b4c4-6d1088b222cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ensemble_return_period.nc ...\n",
      "✅ Saved: ensemble_return_period.csv\n",
      "\n",
      "✅ All NetCDFs converted to CSVs.\n",
      "CSV files saved in:\n",
      "../workspace/Climate Data/Climate Maps CSVs\n"
     ]
    }
   ],
   "source": [
    "# --- SCRIPT 1: Convert all .nc hazard maps into CSVs ---\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from unidecode import unidecode\n",
    "\n",
    "# === PATHS ===\n",
    "nc_dir = \"../tests/tests_data/hazards\"\n",
    "\n",
    "csv_out_dir = \"../workspace/Climate Data/Climate Maps CSVs\"\n",
    "os.makedirs(csv_out_dir, exist_ok=True)\n",
    "\n",
    "# === FILENAME PATTERN PARSER ===\n",
    "# No complex patterns needed - just extract from directory structure\n",
    "\n",
    "def parse_hazard_from_fname(path):\n",
    "    # Extract hazard type and indicator from directory structure\n",
    "    # Path format: .../hazards/Drought/SPI6/ensemble/ensemble_return_period.nc\n",
    "    path_parts = path.split(os.sep)\n",
    "    \n",
    "    # Find the \"hazards\" directory and get the next two parts\n",
    "    hazards_idx = path_parts.index(\"hazards\")\n",
    "    if hazards_idx + 2 >= len(path_parts):\n",
    "        raise ValueError(f\"Invalid path structure: {path}\")\n",
    "    \n",
    "    hazard_type = path_parts[hazards_idx + 1]  # \"Drought\"\n",
    "    hazard_indicator = path_parts[hazards_idx + 2]  # \"SPI6\"\n",
    "    return (hazard_type, hazard_indicator)\n",
    "\n",
    "def nc_to_csv(nc_path, out_dir):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    var_name = list(ds.data_vars.keys())[0]\n",
    "    da = ds[var_name]\n",
    "    \n",
    "    # Expecting dims: ensemble, GWL, return_period, lat, lon\n",
    "    df = da.to_dataframe(name=\"value\").reset_index().dropna(subset=[\"value\"])\n",
    "    \n",
    "    # Add hazard info from filename\n",
    "    h_type, h_ind = parse_hazard_from_fname(nc_path)\n",
    "    df[\"hazard_type\"] = h_type\n",
    "    df[\"hazard_indicator\"] = h_ind\n",
    "    \n",
    "    # Save to CSV\n",
    "    out_name = os.path.splitext(os.path.basename(nc_path))[0] + \".csv\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"✅ Saved: {out_name}\")\n",
    "    return out_path\n",
    "\n",
    "# === RUN ===\n",
    "# Recursively search for .nc files in nc_dir and subdirectories\n",
    "# Look for files containing \"ensemble_return_period\" in the name\n",
    "nc_files = sorted(glob.glob(os.path.join(nc_dir, \"**\", \"*ensemble_return_period*.nc\"), recursive=True))\n",
    "if not nc_files:\n",
    "    raise FileNotFoundError(f\"No .nc files found in {nc_dir} (searched recursively)\")\n",
    "\n",
    "for fp in nc_files:\n",
    "    print(f\"Processing {os.path.basename(fp)} ...\")\n",
    "    nc_to_csv(fp, csv_out_dir)\n",
    "\n",
    "print(\"\\n✅ All NetCDFs converted to CSVs.\")\n",
    "print(f\"CSV files saved in:\\n{csv_out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94e8376-0567-4005-a396-7571bbf4e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating: ensemble_return_period.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/zghzv05d7xb8t9xy7y5ld_h40000gn/T/ipykernel_7654/2613125423.py:110: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/df/zghzv05d7xb8t9xy7y5ld_h40000gn/T/ipykernel_7654/2613125423.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m possible_val:\n\u001b[32m    115\u001b[39m             df = df.rename(columns={possible_val[\u001b[32m0\u001b[39m]: \u001b[33m\"value\"\u001b[39m})\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m     res_adm1 = summarize_points_over_adm(df, adm1, adm1_name, adm_level=\u001b[33m'ADM1'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     res_adm2 = summarize_points_over_adm(df, adm2, adm2_name, adm_level=\u001b[33m'ADM2'\u001b[39m)\n\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m     res_adm1[\u001b[33m\"ensemble\"\u001b[39m] = \u001b[33m\"mean\"\u001b[39m\n\u001b[32m    121\u001b[39m     res_adm2[\u001b[33m\"ensemble\"\u001b[39m] = \u001b[33m\"mean\"\u001b[39m\n",
      "\u001b[32m/var/folders/df/zghzv05d7xb8t9xy7y5ld_h40000gn/T/ipykernel_7654/2613125423.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(points_df, adm_gdf, adm_name_col, adm_level)\u001b[39m\n\u001b[32m     51\u001b[39m         joined.groupby(\n\u001b[32m     52\u001b[39m             [\u001b[33m\"GWL\"\u001b[39m, \u001b[33m\"return_period\"\u001b[39m, \u001b[33m\"ensemble\"\u001b[39m, \u001b[33m\"hazard_type\"\u001b[39m, \u001b[33m\"hazard_indicator\"\u001b[39m, adm_name_col],\n\u001b[32m     53\u001b[39m             dropna=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     54\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         .agg(\n\u001b[32m     56\u001b[39m             min=(\u001b[33m\"value\"\u001b[39m, \u001b[33m\"min\"\u001b[39m),\n\u001b[32m     57\u001b[39m             max=(\u001b[33m\"value\"\u001b[39m, \u001b[33m\"max\"\u001b[39m),\n\u001b[32m     58\u001b[39m             mean=(\u001b[33m\"value\"\u001b[39m, \u001b[33m\"mean\"\u001b[39m),\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1428\u001b[39m             kwargs[\u001b[33m\"engine\"\u001b[39m] = engine\n\u001b[32m   1429\u001b[39m             kwargs[\u001b[33m\"engine_kwargs\"\u001b[39m] = engine_kwargs\n\u001b[32m   1430\u001b[39m \n\u001b[32m   1431\u001b[39m         op = GroupByApply(self, func, args=args, kwargs=kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m         result = op.agg()\n\u001b[32m   1433\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;28;01mand\u001b[39;00m result \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1434\u001b[39m             \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[32m   1435\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.as_index \u001b[38;5;28;01mand\u001b[39;00m is_list_like(func):\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(func, str):\n\u001b[32m    187\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_str()\n\u001b[32m    188\u001b[39m \n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_dict_like()\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m             \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m    193\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_list_like()\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    419\u001b[39m         Returns\n\u001b[32m    420\u001b[39m         -------\n\u001b[32m    421\u001b[39m         Result of aggregation.\n\u001b[32m    422\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.agg_or_apply_dict_like(op_name=\u001b[33m\"agg\"\u001b[39m)\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1599\u001b[39m \n\u001b[32m   1600\u001b[39m         with com.temp_setattr(\n\u001b[32m   1601\u001b[39m             obj, \u001b[33m\"as_index\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=hasattr(obj, \u001b[33m\"as_index\"\u001b[39m)\n\u001b[32m   1602\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m             result_index, result_data = self.compute_dict_like(\n\u001b[32m   1604\u001b[39m                 op_name, selected_obj, selection, kwargs\n\u001b[32m   1605\u001b[39m             )\n\u001b[32m   1606\u001b[39m         result = self.wrap_results_dict_like(selected_obj, result_index, result_data)\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, op_name, selected_obj, selection, kwargs)\u001b[39m\n\u001b[32m    492\u001b[39m                 keys += [key] * len(key_data)\n\u001b[32m    493\u001b[39m                 results += key_data\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    495\u001b[39m             \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m             results = [\n\u001b[32m    497\u001b[39m                 getattr(obj._gotitem(key, ndim=\u001b[32m1\u001b[39m), op_name)(how, **kwargs)\n\u001b[32m    498\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;28;01min\u001b[39;00m func.items()\n\u001b[32m    499\u001b[39m             ]\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    253\u001b[39m             \u001b[38;5;66;03m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[32m    254\u001b[39m             func = maybe_mangle_lambdas(func)\n\u001b[32m    255\u001b[39m             kwargs[\u001b[33m\"engine\"\u001b[39m] = engine\n\u001b[32m    256\u001b[39m             kwargs[\u001b[33m\"engine_kwargs\"\u001b[39m] = engine_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m             ret = self._aggregate_multiple_funcs(func, *args, **kwargs)\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[32m    260\u001b[39m                 \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, arg, *args, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m             \u001b[38;5;66;03m# Combine results using the index, need to adjust index after\u001b[39;00m\n\u001b[32m    359\u001b[39m             \u001b[38;5;66;03m# if as_index=False (GH#50724)\u001b[39;00m\n\u001b[32m    360\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;28;01min\u001b[39;00m enumerate(arg):\n\u001b[32m    361\u001b[39m                 key = base.OutputKey(label=name, position=idx)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m                 results[key] = self.aggregate(func, *args, **kwargs)\n\u001b[32m    363\u001b[39m \n\u001b[32m    364\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m any(isinstance(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m results.values()):\n\u001b[32m    365\u001b[39m             \u001b[38;5;28;01mfrom\u001b[39;00m pandas \u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m                     dtype=obj.dtype,\n\u001b[32m    288\u001b[39m                 )\n\u001b[32m    289\u001b[39m \n\u001b[32m    290\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._grouper.nkeys > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._python_agg_general(func, *args, **kwargs)\n\u001b[32m    292\u001b[39m \n\u001b[32m    293\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    294\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m self._python_agg_general(func, *args, **kwargs)\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m             warn_alias_replacement(self, orig_func, alias)\n\u001b[32m    324\u001b[39m         f = \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, *args, **kwargs)\n\u001b[32m    325\u001b[39m \n\u001b[32m    326\u001b[39m         obj = self._obj_with_exclusions\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         result = self._grouper.agg_series(obj, f)\n\u001b[32m    328\u001b[39m         res = obj._constructor(result, name=obj.name)\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._wrap_aggregated_output(res)\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, obj, func, preserve_dtype)\u001b[39m\n\u001b[32m    860\u001b[39m             \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[32m    861\u001b[39m             \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[32m    862\u001b[39m             preserve_dtype = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m         result = self._aggregate_series_pure_python(obj, func)\n\u001b[32m    865\u001b[39m \n\u001b[32m    866\u001b[39m         npvalues = lib.maybe_convert_objects(result, try_float=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    867\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, obj, func)\u001b[39m\n\u001b[32m    880\u001b[39m         initialized = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    881\u001b[39m \n\u001b[32m    882\u001b[39m         splitter = self._get_splitter(obj, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    883\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;28;01min\u001b[39;00m enumerate(splitter):\n\u001b[32m    885\u001b[39m             res = func(group)\n\u001b[32m    886\u001b[39m             res = extract_result(res)\n\u001b[32m    887\u001b[39m \n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1156\u001b[39m \n\u001b[32m   1157\u001b[39m         starts, ends = lib.generate_slices(self._slabels, self.ngroups)\n\u001b[32m   1158\u001b[39m \n\u001b[32m   1159\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;28;01min\u001b[39;00m zip(starts, ends):\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m self._chop(sdata, slice(start, end))\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/groupby/ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, sdata, slice_obj)\u001b[39m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _chop(self, sdata: Series, slice_obj: slice) -> Series:\n\u001b[32m   1172\u001b[39m         \u001b[38;5;66;03m# fastpath equivalent to `sdata.iloc[slice_obj]`\u001b[39;00m\n\u001b[32m   1173\u001b[39m         mgr = sdata._mgr.get_slice(slice_obj)\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m         ser = sdata._constructor_from_mgr(mgr, axes=mgr.axes)\n\u001b[32m   1175\u001b[39m         ser._name = sdata.name\n\u001b[32m   1176\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ser.__finalize__(sdata, method=\u001b[33m\"groupby\"\u001b[39m)\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, mgr, axes)\u001b[39m\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _constructor_from_mgr(self, mgr, axes):\n\u001b[32m    665\u001b[39m         ser = Series._from_mgr(mgr, axes=axes)\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m         ser._name = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[32m    667\u001b[39m \n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m type(self) \u001b[38;5;28;01mis\u001b[39;00m Series:\n\u001b[32m    669\u001b[39m             \u001b[38;5;66;03m# This would also work `if self._constructor is Series`, but\u001b[39;00m\n",
      "\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6335\u001b[39m \n\u001b[32m   6336\u001b[39m         \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[32m   6337\u001b[39m         \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[32m   6338\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self._internal_names_set:\n\u001b[32m-> \u001b[39m\u001b[32m6339\u001b[39m             object.__setattr__(self, name, value)\n\u001b[32m   6340\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self._metadata:\n\u001b[32m   6341\u001b[39m             object.__setattr__(self, name, value)\n\u001b[32m   6342\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- SCRIPT 2: Aggregate CSVs over ADM1 and ADM2 ---\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from unidecode import unidecode\n",
    "\n",
    "# === PATHS ===\n",
    "csv_dir = csv_out_dir\n",
    "adm1_path = \"../tests/tests_data/areas/province/geoBoundaries-BRA-ADM1.shp\"\n",
    "adm2_path = \"../tests/tests_data/areas/municipality/geoBoundaries-BRA-ADM2.shp\"\n",
    "output_dir = \"../workspace/Climate Data/Precomputed Regional Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"precomputed_adm_nc.csv\")\n",
    "\n",
    "# === LOAD ADM SHAPEFILES ===\n",
    "def load_adm(adm_path):\n",
    "    gdf = gpd.read_file(adm_path)\n",
    "    \n",
    "    # Set CRS if not already set (many shapefiles don't have CRS defined)\n",
    "    if gdf.crs is None:\n",
    "        # Try to infer from .prj file or assume WGS84\n",
    "        gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "    \n",
    "    # Transform to WGS84 if not already\n",
    "    if gdf.crs != \"EPSG:4326\":\n",
    "        gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "\n",
    "    return gdf, \"shapeName\"\n",
    "\n",
    "\n",
    "adm1, adm1_name = load_adm(adm1_path)\n",
    "adm2, adm2_name = load_adm(adm2_path)\n",
    "\n",
    "# === AGGREGATION FUNCTION ===\n",
    "def summarize_points_over_adm(points_df, adm_gdf, adm_name_col, adm_level):\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        points_df,\n",
    "        geometry=gpd.points_from_xy(points_df[\"lon\"], points_df[\"lat\"]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    joined = gpd.sjoin(gdf_pts, adm_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "    def q(p): return lambda x: float(np.nanpercentile(x, p))\n",
    "\n",
    "    agg = (\n",
    "        joined.groupby(\n",
    "            [\"GWL\", \"return_period\", \"ensemble\", \"hazard_type\", \"hazard_indicator\", adm_name_col],\n",
    "            dropna=False,\n",
    "        )\n",
    "        .agg(\n",
    "            min=(\"value\", \"min\"),\n",
    "            max=(\"value\", \"max\"),\n",
    "            mean=(\"value\", \"mean\"),\n",
    "            median=(\"value\", \"median\"),\n",
    "            p2_5=(\"value\", q(2.5)),\n",
    "            p5=(\"value\", q(5)),\n",
    "            p10=(\"value\", q(10)),\n",
    "            p90=(\"value\", q(90)),\n",
    "            p95=(\"value\", q(95)),\n",
    "            p97_5=(\"value\", q(97.5)),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    agg = agg.rename(\n",
    "        columns={\n",
    "            \"GWL\": \"scenario_code\",\n",
    "            \"return_period\": \"hazard_return_period\",\n",
    "            adm_name_col: \"region\",\n",
    "        }\n",
    "    )\n",
    "    agg[\"scenario_name\"] = agg[\"scenario_code\"]\n",
    "    agg[\"adm_level\"] = adm_level\n",
    "\n",
    "    cols = [\n",
    "        \"region\",\n",
    "        \"adm_level\",\n",
    "        \"scenario_code\",\n",
    "        \"scenario_name\",\n",
    "        \"hazard_return_period\",\n",
    "        \"hazard_type\",\n",
    "        \"hazard_indicator\",\n",
    "        \"min\",\n",
    "        \"max\",\n",
    "        \"mean\",\n",
    "        \"median\",\n",
    "        \"p2_5\",\n",
    "        \"p5\",\n",
    "        \"p10\",\n",
    "        \"p90\",\n",
    "        \"p95\",\n",
    "        \"p97_5\",\n",
    "    ]\n",
    "    return agg[cols]\n",
    "\n",
    "# === RUN AGGREGATION ===\n",
    "csv_files = sorted(glob.glob(os.path.join(csv_dir, \"*.csv\")))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSVs found in {csv_dir}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for csv_fp in csv_files:\n",
    "    print(f\"Aggregating: {os.path.basename(csv_fp)}\")\n",
    "    df = pd.read_csv(csv_fp, encoding=\"utf-8\")\n",
    "    df = df[df[\"ensemble\"] == 'mean']\n",
    "    if \"value\" not in df.columns:\n",
    "        possible_val = [c for c in df.columns if c not in [\"GWL\", \"lon\", \"lat\", \"return_period\", \"ensemble\", \"hazard_type\", \"hazard_indicator\"]]\n",
    "        if possible_val:\n",
    "            df = df.rename(columns={possible_val[0]: \"value\"})\n",
    "\n",
    "    res_adm1 = summarize_points_over_adm(df, adm1, adm1_name, adm_level='ADM1')\n",
    "    res_adm2 = summarize_points_over_adm(df, adm2, adm2_name, adm_level='ADM2')\n",
    "\n",
    "    res_adm1[\"ensemble\"] = \"mean\"\n",
    "    res_adm2[\"ensemble\"] = \"mean\"\n",
    "\n",
    "    all_results.extend([res_adm1, res_adm2])\n",
    "    \n",
    "\n",
    "# === COMBINE AND REPAIR ENCODING ===\n",
    "final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# --- FIX MOJIBAKE (e.g. √Ågua → Água) ---\n",
    "\n",
    "# try several encodings automatically\n",
    "\n",
    "def fix_text(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "# Apply only to region names\n",
    "final_df[\"region\"] = final_df[\"region\"].apply(fix_text)\n",
    "\n",
    "# --- ENSURE UTF-8 OUTPUT ---\n",
    "output_path = os.path.join(output_dir, \"precomputed_adm_nc.csv\")\n",
    "final_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n✅ Aggregation complete! Encoding repaired with ftfy.\")\n",
    "print(f\"Saved to:\\n{output_path}\")\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe086c-7b78-4a93-bc17-3f58dfb03a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h100glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h1000glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h10glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h100glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h1000glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h1000glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h100glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_pc_h1000glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h10glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h100glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp26_h1000glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h5glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h25glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h50glob.tif\n",
      "Skipping (not found): ../tests/tests_data/hazards/FloodTIF/global_rcp85_h1000glob.tif\n",
      "✅ Saved flood precompute to:\n",
      "../workspace/Climate Data/Precomputated Regional Data/precomputed_adm_flood.csv\n",
      "     region adm_level scenario_code   scenario_name  hazard_return_period  \\\n",
      "0   Roraima      ADM1       present  CurrentClimate                    10   \n",
      "1     Amapa      ADM1       present  CurrentClimate                    10   \n",
      "2  Amazonas      ADM1       present  CurrentClimate                    10   \n",
      "3      Para      ADM1       present  CurrentClimate                    10   \n",
      "4  Maranhao      ADM1       present  CurrentClimate                    10   \n",
      "\n",
      "  hazard_type hazard_indicator  min     max        mean  median  p2_5    p5  \\\n",
      "0    FloodTIF     Flood Height  1.0  3138.0  446.105197   370.0  18.0  35.0   \n",
      "1    FloodTIF     Flood Height  1.0  4060.0  471.079391   267.0  23.0  43.0   \n",
      "2    FloodTIF     Flood Height  1.0  4393.0  857.913339   775.0  42.0  83.0   \n",
      "3    FloodTIF     Flood Height  1.0  5135.0  986.535755   811.0  26.0  49.0   \n",
      "4    FloodTIF     Flood Height  1.0  3030.0  500.561694   370.0  15.0  29.0   \n",
      "\n",
      "     p10     p90     p95   p97_5  \n",
      "0   69.0   931.0  1168.0  1385.0  \n",
      "1   78.0  1383.0  1764.0  1930.0  \n",
      "2  166.0  1681.0  1902.0  2026.0  \n",
      "3   96.0  2160.0  2289.0  2369.0  \n",
      "4   57.0  1144.0  1391.0  1643.0  \n"
     ]
    }
   ],
   "source": [
    "### add flood hazard\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from unidecode import unidecode\n",
    "\n",
    "# --- INPUTS & OUTPUTS ---\n",
    "\n",
    "# ADM boundaries\n",
    "adm_levels = [\n",
    "    (\"ADM1\", \"../tests/tests_data/areas/province/geoBoundaries-BRA-ADM1.shp\"),\n",
    "    (\"ADM2\", \"../tests/tests_data/areas/municipality/geoBoundaries-BRA-ADM2.shp\"),\n",
    "]\n",
    "\n",
    "# Flood maps directory\n",
    "flood_maps_dir = \"../tests/tests_data/hazards/FloodTIF\"\n",
    "\n",
    "# Scenarios and return periods\n",
    "scenario_codes = [\"pc\", \"rcp26\", \"rcp85\"]\n",
    "scenario_labels = {\"pc\": \"CurrentClimate\", \"rcp26\": \"RCP2.6\", \"rcp85\": \"RCP8.5\"}\n",
    "scenario_code_map_for_output = {\"pc\": \"present\", \"rcp26\": \"rcp26\", \"rcp85\": \"rcp85\"}\n",
    "return_periods = [5, 10, 25, 50, 100, 1000]\n",
    "\n",
    "# Output CSV\n",
    "output_csv = \"../workspace/Climate Data/Precomputed Regional Data/precomputed_adm_flood.csv\"\n",
    "\n",
    "# --- HELPERS ---\n",
    "\n",
    "def fix_text(s):\n",
    "    \"\"\"Try to repair mojibake / encoding artefacts; safe no-op if already fine.\"\"\"\n",
    "    return unidecode(s)\n",
    "\n",
    "def load_adm(adm_path):\n",
    "    \"\"\"Load a shapefile and pick a reasonable region-name column.\"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(adm_path)\n",
    "    except UnicodeDecodeError:\n",
    "        gdf = gpd.read_file(adm_path, encoding=\"latin1\")\n",
    "    # choose name column\n",
    "    name_col = None\n",
    "    for c in [\"shapeName\", \"NAME_2\", \"NAME_1\", \"NAME\", \"name\", \"prov_name\"]:\n",
    "        if c in gdf.columns:\n",
    "            name_col = c\n",
    "            break\n",
    "    if name_col is None:\n",
    "        gdf[\"region\"] = gdf.index.astype(str)\n",
    "    else:\n",
    "        gdf[\"region\"] = gdf[name_col]\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(fix_text)\n",
    "    return gdf\n",
    "\n",
    "def percentile(arr, q):\n",
    "    return float(np.percentile(arr, q)) if arr.size else np.nan\n",
    "\n",
    "# --- PROCESSING ---\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    gdf = load_adm(adm_path)\n",
    "\n",
    "    for sc in scenario_codes:\n",
    "        sc_label = scenario_labels[sc]\n",
    "        sc_code_out = scenario_code_map_for_output[sc]\n",
    "\n",
    "        for rp in return_periods:\n",
    "            tif_name = f\"global_{sc}_h{rp}glob.tif\"\n",
    "            tif_path = os.path.join(flood_maps_dir, tif_name)\n",
    "\n",
    "            # Only process existing .tif files\n",
    "            if not (tif_path.endswith(\".tif\") and os.path.exists(tif_path)):\n",
    "                print(f\"Skipping (not found): {tif_path}\")\n",
    "                continue\n",
    "\n",
    "            with rasterio.open(tif_path) as src:\n",
    "                nodata = src.nodata\n",
    "\n",
    "                # Reproject ADM to raster CRS if needed\n",
    "                if gdf.crs is None:\n",
    "                    gdf = gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "                if gdf.crs != src.crs:\n",
    "                    gdf_proj = gdf.to_crs(src.crs)\n",
    "                else:\n",
    "                    gdf_proj = gdf\n",
    "\n",
    "                for idx, row in gdf_proj.iterrows():\n",
    "                    geom = [row.geometry]\n",
    "                    try:\n",
    "                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Mask error ({adm_label}, {row['region']}, {sc_label}, RP {rp}): {e}\")\n",
    "                        continue\n",
    "\n",
    "                    data = out_image[0]\n",
    "                    if nodata is not None:\n",
    "                        valid_mask = data != nodata\n",
    "                    else:\n",
    "                        valid_mask = np.ones(data.shape, dtype=bool)\n",
    "\n",
    "                    valid = data[valid_mask]\n",
    "                    # If masked array, compress\n",
    "                    if np.ma.isMaskedArray(valid):\n",
    "                        valid = valid.compressed()\n",
    "\n",
    "                    # Compute stats\n",
    "                    if valid.size == 0:\n",
    "                        stats = dict(\n",
    "                            min=np.nan, max=np.nan, mean=np.nan, median=np.nan,\n",
    "                            p2_5=np.nan, p5=np.nan, p10=np.nan, p90=np.nan, p95=np.nan, p97_5=np.nan\n",
    "                        )\n",
    "                    else:\n",
    "                        stats = dict(\n",
    "                            min=float(np.min(valid)),\n",
    "                            max=float(np.max(valid)),\n",
    "                            mean=float(np.mean(valid)),\n",
    "                            median=percentile(valid, 50),\n",
    "                            p2_5=percentile(valid, 2.5),\n",
    "                            p5=percentile(valid, 5),\n",
    "                            p10=percentile(valid, 10),\n",
    "                            p90=percentile(valid, 90),\n",
    "                            p95=percentile(valid, 95),\n",
    "                            p97_5=percentile(valid, 97.5),\n",
    "                        )\n",
    "\n",
    "                    # Build output row (match precomputed_adm_hazards.csv format)\n",
    "                    out_row = {\n",
    "                        \"region\": fix_text(gdf.loc[idx, \"region\"]),   # original name in original CRS GeoDF\n",
    "                        \"adm_level\": adm_label,\n",
    "                        \"scenario_code\": sc_code_out,                 # 'present' | 'rcp26' | 'rcp85'\n",
    "                        \"scenario_name\": sc_label,                    # 'CurrentClimate' | 'RCP2.6' | 'RCP8.5'\n",
    "                        \"hazard_return_period\": rp,\n",
    "                        \"hazard_type\": \"FloodTIF\",\n",
    "                        \"hazard_indicator\": \"Flood Height\",\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"median\": stats[\"median\"],\n",
    "                        \"p2_5\": stats[\"p2_5\"],\n",
    "                        \"p5\": stats[\"p5\"],\n",
    "                        \"p10\": stats[\"p10\"],\n",
    "                        \"p90\": stats[\"p90\"],\n",
    "                        \"p95\": stats[\"p95\"],\n",
    "                        \"p97_5\": stats[\"p97_5\"],\n",
    "                        \"ensemble\": np.nan,\n",
    "                    }\n",
    "                    all_rows.append(out_row)\n",
    "\n",
    "# --- EXPORT ---\n",
    "\n",
    "df_out = pd.DataFrame(all_rows, columns=[\n",
    "    \"region\",\"adm_level\",\"scenario_code\",\"scenario_name\",\"hazard_return_period\",\n",
    "    \"hazard_type\",\"hazard_indicator\",\"min\",\"max\",\"mean\",\"median\",\"p2_5\",\"p5\",\"p10\",\"p90\",\"p95\",\"p97_5\"\n",
    "])\n",
    "\n",
    "# Ensure accent-safe output for Excel (UTF-8 with BOM)\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_out.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Saved flood precompute to:\\n{output_csv}\")\n",
    "print(df_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5f3b4-7c32-485c-9cc2-6f5d405e3469",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../workspace/Climate Data/Precomputated Regional Data/precomputed_adm_nc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m output_file = \u001b[33m\"\u001b[39m\u001b[33m../workspace/Climate Data/Precomputated Regional Data/precomputed_adm_hazards.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# === LOAD FILES (UTF-8 SAFE) ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df_nc = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnc_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8-sig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m df_flood = pd.read_csv(flood_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8-sig\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# === ALIGN COLUMNS ===\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Get the union of all columns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/Theia-Finance-Labs/climate.risk.tool/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../workspace/Climate Data/Precomputated Regional Data/precomputed_adm_nc.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === INPUT FILES ===\n",
    "nc_file = \"../workspace/Climate Data/Precomputed Regional Data/precomputed_adm_nc.csv\"\n",
    "flood_file = \"../workspace/Climate Data/Precomputed Regional Data/precomputed_adm_flood.csv\"\n",
    "\n",
    "# === OUTPUT FILE ===\n",
    "output_file = \"../workspace/Climate Data/Precomputed Regional Data/precomputed_adm_hazards.csv\"\n",
    "\n",
    "# === LOAD FILES (UTF-8 SAFE) ===\n",
    "df_nc = pd.read_csv(nc_file, encoding=\"utf-8-sig\")\n",
    "df_flood = pd.read_csv(flood_file, encoding=\"utf-8-sig\")\n",
    "\n",
    "# === ALIGN COLUMNS ===\n",
    "# Get the union of all columns\n",
    "all_cols = sorted(set(df_nc.columns).union(set(df_flood.columns)))\n",
    "\n",
    "# Add missing columns as NaN so both match perfectly\n",
    "for df in [df_nc, df_flood]:\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    df = df[all_cols]\n",
    "\n",
    "# === MERGE (STACK) ===\n",
    "df_merged = pd.concat([df_nc, df_flood], ignore_index=True)\n",
    "\n",
    "# === SAVE OUTPUT ===\n",
    "df_merged.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Merged dataset saved successfully to:\\n{output_file}\")\n",
    "print(f\"Total records: {len(df_merged)}\")\n",
    "print(\"Columns:\", list(df_merged.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a887b-ce10-4ee4-bd9f-73843bb869d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
