{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab4d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] ../workspace/GIRI_raw/flood_pc_500_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_200_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_100_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_50_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_25_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_5_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_pc_2_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_500_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_200_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_100_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_50_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_25_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_5_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp85_2_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_500_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_200_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_100_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_50_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_25_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_5_glob.tif already exists.\n",
      "[SKIP] ../workspace/GIRI_raw/flood_rcp26_2_glob.tif already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "scenarios = [\"pc\", \"rcp85\", \"rcp26\"]\n",
    "return_periods = [500, 200, 100, 50, 25, 5, 2]\n",
    "\n",
    "folderr = \"../workspace/GIRI_raw/\"\n",
    "os.makedirs(folderr, exist_ok=True)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for rp in return_periods:\n",
    "        url = f\"https://hazards-data.unepgrid.ch/global_{scenario}_h{rp}glob.tif\"\n",
    "        filename = os.path.join(folderr, f\"flood_{scenario}_{rp}_glob.tif\")\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"[SKIP] {filename} already exists.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] Downloading {filename} ...\")\n",
    "        try:\n",
    "            r = requests.get(url, timeout=60)\n",
    "            if r.status_code == 200:\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"[OK] Saved {filename}\")\n",
    "            else:\n",
    "                print(f\"[WARN] URL not found or unavailable: {url} (status {r.status_code})\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] Failed to download {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1d95d",
   "metadata": {},
   "source": [
    "# setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f31aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 21/21 [00:46<00:00,  2.20s/file, status=Loading: rcp26_2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Combining arrays…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining: 100%|██████████| 1/1 [02:15<00:00, 135.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Transposing dimensions…\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr  # make sure: pip/conda install rioxarray\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- config ----------------\n",
    "scenarios = [\"pc\", \"rcp85\", \"rcp26\"]\n",
    "return_periods = [500, 200, 100, 50, 25, 5, 2]\n",
    "\n",
    "# map to human-readable names (as a coord)\n",
    "dict_scenarios = {\n",
    "    \"pc\": \"present\",\n",
    "    \"rcp85\": \"rcp85\",\n",
    "    \"rcp26\": \"rcp26\",\n",
    "}\n",
    "\n",
    "# crop extent (lon_min, lon_max, lat_max, lat_min)\n",
    "LON_MIN, LON_MAX = -75.0, -30.0\n",
    "LAT_MAX, LAT_MIN = 10.0, -35.0\n",
    "\n",
    "# chunk sizes for dask/xarray and NetCDF\n",
    "CHUNK_LAT, CHUNK_LON = 512, 512\n",
    "\n",
    "# ----------------------------------------\n",
    "list_da = []\n",
    "\n",
    "# Create total count for progress bar\n",
    "total_files = len(scenarios) * len(return_periods)\n",
    "pbar = tqdm(total=total_files, desc=\"Loading files\", unit=\"file\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for rp in return_periods:\n",
    "        fpath = os.path.join(folderr, f\"flood_{scenario}_{rp}_glob.tif\")\n",
    "        \n",
    "        if not os.path.exists(fpath):\n",
    "            pbar.set_postfix({\"status\": f\"Missing: {scenario}_{rp}\"})\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        pbar.set_postfix({\"status\": f\"Loading: {scenario}_{rp}\"})\n",
    "        \n",
    "        try:\n",
    "            da = rxr.open_rasterio(\n",
    "                fpath,\n",
    "                chunks={\"x\": CHUNK_LON, \"y\": CHUNK_LAT},\n",
    "                masked=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pbar.set_postfix({\"status\": f\"ERROR: {scenario}_{rp}\"})\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        # squeeze band -> 2D (y,x)\n",
    "        if \"band\" in da.dims and da.sizes[\"band\"] == 1:\n",
    "            da = da.squeeze(\"band\", drop=True)\n",
    "\n",
    "        # ensure CRS; many global hazard tifs are EPSG:4326\n",
    "        if da.rio.crs is None:\n",
    "            da = da.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "        # crop (note: y usually descends; slice handles either order)\n",
    "        da = da.sel(x=slice(LON_MIN, LON_MAX), y=slice(LAT_MAX, LAT_MIN))\n",
    "\n",
    "        # rename dims/var\n",
    "        da = da.rename({\"x\": \"lon\", \"y\": \"lat\"})\n",
    "        da.name = \"flood_depth\"\n",
    "\n",
    "        # set dtype (float32 is compact and typical for depths)\n",
    "        da = da.astype(\"float32\")\n",
    "\n",
    "        # expand coords with scenario + return_period\n",
    "        da = da.expand_dims(\n",
    "            {\n",
    "                \"GWL\": [dict_scenarios.get(scenario, scenario)],\n",
    "                \"return_period\": [rp],\n",
    "                \"ensemble\": [\"mean\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        list_da.append(da)\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# guard: anything loaded?\n",
    "if not list_da:\n",
    "    raise RuntimeError(\"No rasters were found/loaded. Check file names and folder.\")\n",
    "\n",
    "# combine by coords into one cube\n",
    "print(\"\\n[INFO] Combining arrays…\")\n",
    "with tqdm(total=1, desc=\"Combining\") as pbar:\n",
    "    da_all = xr.combine_by_coords(list_da, combine_attrs=\"override\")\n",
    "    pbar.update(1)\n",
    "\n",
    "# order dims - IMPORTANT: This order matches load_nc_hazards_with_metadata expectations\n",
    "# The function expects: (ensemble, GWL, return_period, lat, lon)\n",
    "print(\"[INFO] Transposing dimensions…\")\n",
    "\n",
    "# combine_by_coords returns a Dataset when all DataArrays have the same name\n",
    "# Just transpose it directly\n",
    "ds = da_all.transpose(\"ensemble\", \"GWL\", \"return_period\", \"lat\", \"lon\")\n",
    "\n",
    "# set some attrs\n",
    "ds[\"flood_depth\"].attrs.update(\n",
    "    {\n",
    "        \"long_name\": \"Flood depth\",\n",
    "        \"units\": \"m\",\n",
    "        \"source\": \"UNEP GRID hazards data\",\n",
    "        \"note\": \"Cropped to South America; scenarios labelled by human-readable names.\",\n",
    "    }\n",
    ")\n",
    "ds[\"GWL\"].attrs[\"description\"] = \"Scenario label\"\n",
    "ds[\"return_period\"].attrs[\"units\"] = \"years\"\n",
    "ds[\"ensemble\"].attrs[\"description\"] = \"Ensemble statistics\"\n",
    "\n",
    "# rechunk uniformly for writing\n",
    "ds = ds.chunk({ \"ensemble\": 1, \"GWL\": 1, \"return_period\": 1, \"lat\": CHUNK_LAT, \"lon\": CHUNK_LON})\n",
    "\n",
    "# encode & save\n",
    "encoding = {\n",
    "    \"flood_depth\": {\n",
    "        \"zlib\": True,\n",
    "        \"complevel\": 4,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"chunksizes\": (1, 1, 1, CHUNK_LAT, CHUNK_LON),  # Note: 5 dims now (ensemble, GWL, return_period, lat, lon)\n",
    "        \"_FillValue\": np.float32(np.nan),  # keep NaN for no-data\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f28682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Writing NetCDF -> ../workspace/hazards/Flood/depth/ensemble_return_period.nc\n",
      "[INFO] This may take several minutes...\n",
      "\n",
      "[OK] Done! File written successfully.\n"
     ]
    }
   ],
   "source": [
    "out_nc = \"../workspace/hazards/Flood/depth/ensemble_return_period.nc\"\n",
    "print(f\"\\n[INFO] Writing NetCDF -> {out_nc}\")\n",
    "print(\"[INFO] This may take several minutes...\")\n",
    "ds.to_netcdf(out_nc, encoding=encoding)\n",
    "print(\"\\n[OK] Done! File written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
