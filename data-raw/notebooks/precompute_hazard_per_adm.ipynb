{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ce320a-5aab-4852-89c5-b8f43b91d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import rasterio.transform\n",
    "from rasterio.io import MemoryFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "from unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33502dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ADM1 boundaries from ../../workspace/boundaries/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp ...\n",
      "  Applying encoding fixes to ADM1 region names...\n",
      "  ✓ Applied encoding fixes to all 27 region names in ADM1\n",
      "  ✓ All region names were already properly encoded\n",
      "  ✓ Loaded ADM1: 27 regions\n",
      "Reading ADM2 boundaries from ../../workspace/boundaries/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp ...\n",
      "  Applying encoding fixes to ADM2 region names...\n",
      "  ✓ Applied encoding fixes to all 5570 region names in ADM2\n",
      "  Examples of changes:\n",
      "    'EspigÃ£o D'Oeste' -> 'Espigao D'Oeste'\n",
      "    'GuajarÃ¡-Mirim' -> 'Guajara-Mirim'\n",
      "    'Ji-ParanÃ¡' -> 'Ji-Parana'\n",
      "    'Nova BrasilÃ¢ndia D'Oeste' -> 'Nova Brasilandia D'Oeste'\n",
      "    'Presidente MÃ©dici' -> 'Presidente Medici'\n",
      "    ... and 2380 more\n",
      "  ✓ Loaded ADM2: 5570 regions\n"
     ]
    }
   ],
   "source": [
    "# --- ENCODING FIX FUNCTIONS ---\n",
    "\n",
    "def reverse_corruption_and_unidecode(corrupted_name):\n",
    "    \"\"\"\n",
    "    Reverse the latin1->utf8 corruption and then unidecode.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reverse the corruption: encode as latin1, decode as utf-8\n",
    "        original_name = corrupted_name.encode(\"latin1\").decode(\"utf-8\")\n",
    "        # Now unidecode the original name\n",
    "        unidecoded_name = unidecode(original_name)\n",
    "        return unidecoded_name\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        # If that fails, try direct unidecode\n",
    "        return unidecode(corrupted_name)\n",
    "\n",
    "\n",
    "def fix_region_encoding(region_name):\n",
    "    \"\"\"\n",
    "    Fix encoding issues in region names by applying reverse corruption + unidecode to ALL names.\n",
    "    This ensures consistent encoding and removes special characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(region_name, str):\n",
    "        return region_name\n",
    "    \n",
    "    # Apply encoding fix to ALL region names for consistency\n",
    "    return reverse_corruption_and_unidecode(region_name)\n",
    "\n",
    "\n",
    "# --- INPUTS & OUTPUTS ---\n",
    "##Input here the ADM boundaries ADM1= province, ADM2= Municipality\n",
    "# ADM boundaries\n",
    "adm_levels = [\n",
    "    (\"ADM1\", \"../../workspace/boundaries/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp\"),\n",
    "    (\"ADM2\", \"../../workspace/boundaries/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp\")\n",
    "]\n",
    "\n",
    "# Hazards root directory (contains both TIF and NC files)\n",
    "hazards_root_dir = \"../../workspace/demo_inputs/hazards\"\n",
    "\n",
    "# TIF-specific configuration\n",
    "tif_subdir = \"flood\"  # Subdirectory within hazards_root_dir for TIF files\n",
    "scenario_codes = [\"pc\", \"rcp26\", \"rcp85\"]\n",
    "scenario_labels = {\n",
    "    \"pc\": \"CurrentClimate\",\n",
    "    \"rcp26\": \"RCP2.6\",\n",
    "    \"rcp85\": \"RCP8.5\"\n",
    "}\n",
    "return_periods = [10, 100, 1000]\n",
    "\n",
    "# NC files will be auto-discovered recursively in hazards_root_dir\n",
    "# Expected structure: hazards/{hazard_type}/{hazard_indicator}/{model_type}/{file}.nc\n",
    "\n",
    "# Output directory for individual file results\n",
    "output_dir = \"../../workspace/precomputed_region_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Final combined output CSV path\n",
    "output_csv = \"../../workspace/precomputed_adm_hazards.csv\"\n",
    "\n",
    "adm_gdfs = {}\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    print(f\"Reading {adm_label} boundaries from {adm_path} ...\")\n",
    "    if not os.path.exists(adm_path):\n",
    "        raise FileNotFoundError(f\"Boundary file {adm_path} not found. Please ensure the ADM boundary shapefiles are available.\")\n",
    "    gdf = gpd.read_file(adm_path)\n",
    "    # Standardize column names for code and name fields\n",
    "    gdf.columns = [col.lower() for col in gdf.columns]\n",
    "    # For ADM1, geoBoundaries usually uses shapeID or shapeName columns\n",
    "    if adm_label == \"ADM1\":\n",
    "        # Try to set 'adm_code' and 'adm_name'\n",
    "        # Common geoBoundaries columns: shapeName (name), shapeID (code), sometimes ADM1_PCODE, etc.\n",
    "        if \"shapeid\" in gdf.columns:\n",
    "            gdf[\"adm_code\"] = gdf[\"shapeid\"]\n",
    "        elif \"adm1_pcode\" in gdf.columns:\n",
    "            gdf[\"adm_code\"] = gdf[\"adm1_pcode\"]\n",
    "        else:\n",
    "            raise ValueError(f\"ADM1 boundary missing unique code column (e.g. shapeID, ADM1_PCODE): columns={gdf.columns}\")\n",
    "\n",
    "        if \"shapename\" in gdf.columns:\n",
    "            gdf[\"adm_name\"] = gdf[\"shapename\"]\n",
    "        elif \"adm1_fr\" in gdf.columns:\n",
    "            gdf[\"adm_name\"] = gdf[\"adm1_fr\"]\n",
    "        else:\n",
    "            raise ValueError(f\"ADM1 boundary missing name column (e.g. shapeName, ADM1_FR): columns={gdf.columns}\")\n",
    "    elif adm_label == \"ADM2\":\n",
    "        # Try to set 'adm_code' and 'adm_name'\n",
    "        if \"shapeid\" in gdf.columns:\n",
    "            gdf[\"adm_code\"] = gdf[\"shapeid\"]\n",
    "        elif \"adm2_pcode\" in gdf.columns:\n",
    "            gdf[\"adm_code\"] = gdf[\"adm2_pcode\"]\n",
    "        else:\n",
    "            raise ValueError(f\"ADM2 boundary missing unique code column (e.g. shapeID, ADM2_PCODE): columns={gdf.columns}\")\n",
    "\n",
    "        if \"shapename\" in gdf.columns:\n",
    "            gdf[\"adm_name\"] = gdf[\"shapename\"]\n",
    "        elif \"adm2_fr\" in gdf.columns:\n",
    "            gdf[\"adm_name\"] = gdf[\"adm2_fr\"]\n",
    "        else:\n",
    "            raise ValueError(f\"ADM2 boundary missing name column (e.g. shapeName, ADM2_FR): columns={gdf.columns}\")\n",
    "    \n",
    "    # Set the region column for use in processing\n",
    "    gdf[\"region\"] = gdf[\"adm_name\"]\n",
    "    \n",
    "    # Apply encoding fixes to region names\n",
    "    print(f\"  Applying encoding fixes to {adm_label} region names...\")\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(fix_region_encoding)\n",
    "    \n",
    "    # Show examples of fixes applied\n",
    "    original_names = gdf[\"adm_name\"].tolist()\n",
    "    fixed_names = gdf[\"region\"].tolist()\n",
    "    fixes_applied = [(orig, fixed) for orig, fixed in zip(original_names, fixed_names) if orig != fixed]\n",
    "    \n",
    "    print(f\"  ✓ Applied encoding fixes to all {len(gdf)} region names in {adm_label}\")\n",
    "    if fixes_applied:\n",
    "        print(\"  Examples of changes:\")\n",
    "        for i, (orig, fixed) in enumerate(fixes_applied[:5]):\n",
    "            print(f\"    '{orig}' -> '{fixed}'\")\n",
    "        if len(fixes_applied) > 5:\n",
    "            print(f\"    ... and {len(fixes_applied) - 5} more\")\n",
    "    else:\n",
    "        print(\"  ✓ All region names were already properly encoded\")\n",
    "\n",
    "    adm_gdfs[adm_label] = gdf\n",
    "    print(f\"  ✓ Loaded {adm_label}: {len(gdf)} regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095bf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NC TO RASTER CONVERSION HELPER ---\n",
    "\n",
    "def nc_slice_to_raster(nc_file, gwl_idx, rp_idx, ensemble_value='mean'):\n",
    "    \"\"\"\n",
    "    Convert a NetCDF slice to a rasterio-compatible in-memory raster.\n",
    "    \n",
    "    Uses the same cell-center to cell-edge conversion logic as R code:\n",
    "    - Extract lon/lat coordinates (cell centers)\n",
    "    - Calculate resolution: (max - min) / (n - 1)\n",
    "    - Extend extent by half-pixel: xmin = min(lon) - res/2, xmax = max(lon) + res/2\n",
    "    \n",
    "    Args:\n",
    "        nc_file: Path to NetCDF file\n",
    "        gwl_idx: Index for GWL dimension (0-based)\n",
    "        rp_idx: Index for return_period dimension (0-based)\n",
    "        ensemble_value: Value to filter for ensemble dimension (default: 'mean')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (rasterio dataset in memory, metadata dict)\n",
    "    \"\"\"\n",
    "    dataset = nc.Dataset(nc_file, 'r')\n",
    "    \n",
    "    # Find the main data variable (prefer one ending with '_max')\n",
    "    var_names = [v for v in dataset.variables.keys() \n",
    "                 if v not in ['lat', 'lon', 'latitude', 'longitude', 'GWL', 'gwl', 'return_period', 'ensemble']]\n",
    "    main_var = None\n",
    "    for v in var_names:\n",
    "        if v.endswith('_max'):\n",
    "            main_var = v\n",
    "            break\n",
    "    if main_var is None and var_names:\n",
    "        main_var = var_names[0]\n",
    "    \n",
    "    if main_var is None:\n",
    "        dataset.close()\n",
    "        return None, None\n",
    "    \n",
    "    var = dataset.variables[main_var]\n",
    "    dim_names = [d for d in var.dimensions]\n",
    "    \n",
    "    # Identify dimension names (case-insensitive)\n",
    "    lon_dim = next((d for d in dim_names if d.lower() in ['lon', 'longitude', 'x']), 'lon')\n",
    "    lat_dim = next((d for d in dim_names if d.lower() in ['lat', 'latitude', 'y']), 'lat')\n",
    "    ens_dim = next((d for d in dim_names if d.lower() == 'ensemble'), None)\n",
    "    gwl_dim = next((d for d in dim_names if d.lower() in ['gwl', 'GWL']), None)\n",
    "    \n",
    "    # Remaining dimension is likely return_period\n",
    "    rp_dim = next((d for d in dim_names \n",
    "                   if d not in [lon_dim, lat_dim, ens_dim, gwl_dim]), 'return_period')\n",
    "    \n",
    "    # Get coordinate values\n",
    "    lon_vals = dataset.variables[lon_dim][:]\n",
    "    lat_vals = dataset.variables[lat_dim][:]\n",
    "    \n",
    "    # Find ensemble index\n",
    "    ens_idx = 0\n",
    "    if ens_dim and ens_dim in dataset.variables:\n",
    "        ens_vals = dataset.variables[ens_dim][:]\n",
    "        if ens_vals.dtype.kind in ['U', 'S']:  # String type\n",
    "            ens_list = [str(e) if isinstance(e, bytes) else e for e in ens_vals]\n",
    "            if ensemble_value in ens_list:\n",
    "                ens_idx = ens_list.index(ensemble_value)\n",
    "    \n",
    "    # Build slice indices\n",
    "    slice_dict = {}\n",
    "    for d in dim_names:\n",
    "        if d == lon_dim or d == lat_dim:\n",
    "            slice_dict[d] = slice(None)\n",
    "        elif d == ens_dim:\n",
    "            slice_dict[d] = ens_idx\n",
    "        elif d == gwl_dim:\n",
    "            slice_dict[d] = gwl_idx\n",
    "        elif d == rp_dim:\n",
    "            slice_dict[d] = rp_idx\n",
    "        else:\n",
    "            slice_dict[d] = 0\n",
    "    \n",
    "    # Extract 2D slice\n",
    "    # Build proper indexing tuple based on dimension order\n",
    "    idx_tuple = tuple(slice_dict[d] for d in dim_names)\n",
    "    data_slice = var[idx_tuple]\n",
    "    \n",
    "    # Ensure 2D\n",
    "    if data_slice.ndim > 2:\n",
    "        data_slice = np.squeeze(data_slice)\n",
    "    \n",
    "    # Calculate resolution and extent (cell-center to cell-edge conversion)\n",
    "    n_lon = len(lon_vals)\n",
    "    n_lat = len(lat_vals)\n",
    "    \n",
    "    res_lon = (np.max(lon_vals) - np.min(lon_vals)) / (n_lon - 1) if n_lon > 1 else 1.0\n",
    "    res_lat = (np.max(lat_vals) - np.min(lat_vals)) / (n_lat - 1) if n_lat > 1 else 1.0\n",
    "    \n",
    "    # Extend by half-pixel\n",
    "    xmin = np.min(lon_vals) - res_lon / 2\n",
    "    xmax = np.max(lon_vals) + res_lon / 2\n",
    "    ymin = np.min(lat_vals) - res_lat / 2\n",
    "    ymax = np.max(lat_vals) + res_lat / 2\n",
    "    \n",
    "    # Ensure correct orientation: if data is (lon, lat), transpose to (lat, lon)\n",
    "    if data_slice.shape[0] == n_lon and data_slice.shape[1] == n_lat:\n",
    "        data_slice = data_slice.T\n",
    "    \n",
    "    # Flip vertically so first row is max(lat)\n",
    "    data_slice = np.flipud(data_slice)\n",
    "    \n",
    "    # Create transform\n",
    "    transform = rasterio.transform.from_bounds(xmin, ymin, xmax, ymax, n_lon, n_lat)\n",
    "    \n",
    "    # Create in-memory raster\n",
    "    memfile = MemoryFile()\n",
    "    with memfile.open(\n",
    "        driver='GTiff',\n",
    "        height=n_lat,\n",
    "        width=n_lon,\n",
    "        count=1,\n",
    "        dtype=data_slice.dtype,\n",
    "        crs='EPSG:4326',\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(data_slice, 1)\n",
    "    \n",
    "    # Get metadata for inventory\n",
    "    gwl_vals = dataset.variables[gwl_dim][:] if gwl_dim and gwl_dim in dataset.variables else [gwl_idx]\n",
    "    rp_vals = dataset.variables[rp_dim][:] if rp_dim in dataset.variables else [rp_idx]\n",
    "    \n",
    "    gwl_label = str(gwl_vals[gwl_idx]) if gwl_idx < len(gwl_vals) else f\"idx{gwl_idx}\"\n",
    "    rp_label = str(rp_vals[rp_idx]) if rp_idx < len(rp_vals) else f\"idx{rp_idx}\"\n",
    "    \n",
    "    metadata = {\n",
    "        'gwl': gwl_label,\n",
    "        'return_period': rp_label,\n",
    "        'main_var': main_var\n",
    "    }\n",
    "    \n",
    "    dataset.close()\n",
    "    \n",
    "    # Return the memfile (stays open) and metadata\n",
    "    return memfile, metadata\n",
    "\n",
    "\n",
    "def discover_nc_files(hazards_dir):\n",
    "    \"\"\"\n",
    "    Discover all NC files and extract their metadata from path structure.\n",
    "    Expected: hazards/{hazard_type}/{hazard_indicator}/{model_type}/{file}.nc\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with file path and parsed metadata\n",
    "    \"\"\"\n",
    "    nc_files = glob.glob(os.path.join(hazards_dir, '**', '*.nc'), recursive=True)\n",
    "    \n",
    "    results = []\n",
    "    for f in nc_files:\n",
    "        # Parse path components\n",
    "        parts = os.path.normpath(f).split(os.sep)\n",
    "        \n",
    "        if len(parts) >= 4:\n",
    "            file_name = parts[-1]\n",
    "            model_type = parts[-2]\n",
    "            hazard_indicator = parts[-3]\n",
    "            hazard_type = parts[-4]\n",
    "        else:\n",
    "            # Fallback\n",
    "            file_name = os.path.basename(f)\n",
    "            model_type = 'ensemble'\n",
    "            hazard_indicator = 'indicator'\n",
    "            hazard_type = 'unknown'\n",
    "        \n",
    "        results.append({\n",
    "            'path': f,\n",
    "            'hazard_type': hazard_type,\n",
    "            'hazard_indicator': hazard_indicator,\n",
    "            'model_type': model_type,\n",
    "            'file_name': file_name\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6373b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART 2: Processing NetCDF files...\n",
      "============================================================\n",
      "Found 2 NetCDF file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing demo_inputs/hazards:  50%|█████     | 1/2 [04:11<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Error processing GIRI_flood_depth_cube.nc: a6a84398-cac6-456e-ad40-69a4ebff734b.tif: Free disk space available is 17179869184 bytes, whereas 23328000000 are at least necessary. You can disable this check by defining the CHECK_DISK_FREE_SPACE configuration option to FALSE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [11:46<04:11, 251.96s/file]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwlpresent_rp5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [27:47<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwlpresent_rp10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [30:43<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwlpresent_rp25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [33:23<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwlpresent_rp50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [36:13<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwlpresent_rp100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [39:09<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl1.5_rp5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [42:07<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl1.5_rp10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [45:04<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl1.5_rp25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [48:01<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl1.5_rp50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [50:58<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl1.5_rp100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [53:55<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl2_rp5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [56:55<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl2_rp10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:03:04<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl2_rp25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:18:02<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl2_rp50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:25:17<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl2_rp100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:34:00<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl3_rp5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:42:29<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl3_rp10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:48:00<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl3_rp25.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6:  50%|█████     | 1/2 [1:52:38<04:11, 251.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl3_rp50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drought/SPI6: 100%|██████████| 2/2 [2:03:11<00:00, 3695.52s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved 5597 records to Drought_SPI6_gwl3_rp100.csv\n",
      "\n",
      "✓ NC processing complete!\n",
      "\n",
      "============================================================\n",
      "All hazard processing complete!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PROCESSING NC FILES ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 2: Processing NetCDF files...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Discover all NC files\n",
    "nc_files_info = discover_nc_files(hazards_root_dir)\n",
    "\n",
    "if not nc_files_info:\n",
    "    print(\"⚠ No NetCDF files found. Skipping NC processing.\")\n",
    "else:\n",
    "    print(f\"Found {len(nc_files_info)} NetCDF file(s)\")\n",
    "    \n",
    "    # Create progress bar for NC files\n",
    "    pbar_nc = tqdm(total=len(nc_files_info), desc=\"Processing NC files\", unit=\"file\", position=0, leave=True, dynamic_ncols=True)\n",
    "    \n",
    "    for nc_info in nc_files_info:\n",
    "        nc_path = nc_info['path']\n",
    "        hazard_type = nc_info['hazard_type']\n",
    "        hazard_indicator = nc_info['hazard_indicator']\n",
    "        \n",
    "        pbar_nc.set_description(f\"Processing {hazard_type}/{hazard_indicator}\")\n",
    "        \n",
    "        # Open NC file to discover dimensions\n",
    "        try:\n",
    "            dataset = nc.Dataset(nc_path, 'r')\n",
    "            \n",
    "            # Find dimensions\n",
    "            dim_names = list(dataset.dimensions.keys())\n",
    "            gwl_dim = next((d for d in dim_names if d.lower() in ['gwl', 'GWL']), None)\n",
    "            rp_dim = next((d for d in dim_names if d.lower() in ['return_period']), None)\n",
    "            \n",
    "            # Get dimension sizes\n",
    "            n_gwl = len(dataset.dimensions[gwl_dim]) if gwl_dim else 1\n",
    "            n_rp = len(dataset.dimensions[rp_dim]) if rp_dim else 1\n",
    "            \n",
    "            # Get actual values for labeling\n",
    "            gwl_vals = dataset.variables[gwl_dim][:] if gwl_dim and gwl_dim in dataset.variables else list(range(n_gwl))\n",
    "            rp_vals = dataset.variables[rp_dim][:] if rp_dim and rp_dim in dataset.variables else list(range(n_rp))\n",
    "            \n",
    "            dataset.close()\n",
    "            \n",
    "            # Process each GWL x return_period combination\n",
    "            for ig in range(n_gwl):\n",
    "                for ir in range(n_rp):\n",
    "                    gwl_label = str(gwl_vals[ig]) if ig < len(gwl_vals) else f\"idx{ig}\"\n",
    "                    rp_label = str(rp_vals[ir]) if ir < len(rp_vals) else f\"idx{ir}\"\n",
    "                    \n",
    "                    # Check if output already exists\n",
    "                    output_filename = f\"{hazard_type}_{hazard_indicator}_gwl{gwl_label}_rp{rp_label}.csv\"\n",
    "                    output_path = os.path.join(output_dir, output_filename)\n",
    "                    \n",
    "                    if os.path.exists(output_path):\n",
    "                        pbar_nc.write(f\"⏭  Already processed: {output_filename}. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # NC files contain pre-computed statistics in ensemble dimension\n",
    "                    # Extract each ensemble value separately instead of computing from mean\n",
    "                    \n",
    "                    # Detect available ensemble values\n",
    "                    try:\n",
    "                        test_ds = nc.Dataset(nc_path, 'r')\n",
    "                        dim_names = list(test_ds.dimensions.keys())\n",
    "                        ens_dim = next((d for d in dim_names if d.lower() == 'ensemble'), None)\n",
    "                        \n",
    "                        if ens_dim and ens_dim in test_ds.variables:\n",
    "                            ens_vals = test_ds.variables[ens_dim][:]\n",
    "                            if ens_vals.dtype.kind in ['U', 'S']:  # String type\n",
    "                                ensemble_values = [str(e) if isinstance(e, bytes) else e for e in ens_vals]\n",
    "                            else:\n",
    "                                ensemble_values = ['mean']  # Fallback\n",
    "                        else:\n",
    "                            ensemble_values = ['mean']  # No ensemble dimension\n",
    "                        \n",
    "                        test_ds.close()\n",
    "                    except Exception as e:\n",
    "                        pbar_nc.write(f\"⚠ Could not detect ensemble values: {e}\")\n",
    "                        ensemble_values = ['mean']\n",
    "                    \n",
    "                    # Map ensemble values to statistic columns\n",
    "                    ensemble_to_stat = {\n",
    "                        'mean': 'mean',\n",
    "                        'median': 'median',\n",
    "                        'p10': 'p10',\n",
    "                        'p90': 'p90',\n",
    "                        'p2.5': 'p2_5',\n",
    "                        'p5': 'p5',\n",
    "                        'p95': 'p95',\n",
    "                        'p97.5': 'p97_5'\n",
    "                    }\n",
    "                    \n",
    "                    # Results for this slice - one row per region\n",
    "                    file_results = []\n",
    "                    \n",
    "                    # Process each ADM level\n",
    "                    for adm_label, gdf in adm_gdfs.items():\n",
    "                        # Loop through each polygon\n",
    "                        for idx, row in gdf.iterrows():\n",
    "                            geom = [row[\"geometry\"]]\n",
    "                            \n",
    "                            # Initialize result for this region\n",
    "                            result = {\n",
    "                                \"region\": row[\"region\"],\n",
    "                                \"adm_level\": adm_label,\n",
    "                                \"scenario_code\": gwl_label,\n",
    "                                \"scenario_name\": gwl_label,\n",
    "                                \"hazard_return_period\": rp_label,\n",
    "                                \"hazard_type\": hazard_type,\n",
    "                                \"hazard_indicator\": hazard_indicator,\n",
    "                                \"min\": np.nan,\n",
    "                                \"max\": np.nan,\n",
    "                                \"mean\": np.nan,\n",
    "                                \"median\": np.nan,\n",
    "                                \"p2_5\": np.nan,\n",
    "                                \"p5\": np.nan,\n",
    "                                \"p95\": np.nan,\n",
    "                                \"p97_5\": np.nan,\n",
    "                                \"n_obs\": 0,\n",
    "                                \"max_x\": np.nan,\n",
    "                                \"max_y\": np.nan\n",
    "                            }\n",
    "                            \n",
    "                            # Extract each ensemble value separately\n",
    "                            for ens_value in ensemble_values:\n",
    "                                # Convert NC slice to raster for this specific ensemble\n",
    "                                memfile, metadata = nc_slice_to_raster(nc_path, ig, ir, ensemble_value=ens_value)\n",
    "                                \n",
    "                                if memfile is None:\n",
    "                                    continue\n",
    "                                \n",
    "                                try:\n",
    "                                    with memfile.open() as src:\n",
    "                                        nodata_val = src.nodata\n",
    "                                        \n",
    "                                        # Mask and crop the raster\n",
    "                                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                                        data = out_image[0]\n",
    "                                        \n",
    "                                        # Create mask for valid data\n",
    "                                        if nodata_val is not None:\n",
    "                                            valid_mask = data != nodata_val\n",
    "                                            valid_data = data[valid_mask]\n",
    "                                        else:\n",
    "                                            valid_mask = np.ones(data.shape, dtype=bool)\n",
    "                                            valid_data = data\n",
    "                                        \n",
    "                                        # Handle masked arrays\n",
    "                                        if np.ma.is_masked(valid_data):\n",
    "                                            valid_data = valid_data.compressed()\n",
    "                                        \n",
    "                                        n_obs = valid_data.size\n",
    "                                        \n",
    "                                        if n_obs > 0:\n",
    "                                            # Extract mean of this ensemble layer  \n",
    "                                            extracted_value = float(np.mean(valid_data))\n",
    "                                            \n",
    "                                            # Map to appropriate statistic\n",
    "                                            stat_name = ensemble_to_stat.get(ens_value, ens_value)\n",
    "                                            if stat_name in result:\n",
    "                                                result[stat_name] = extracted_value\n",
    "                                            \n",
    "                                            # Track n_obs from first ensemble\n",
    "                                            if result[\"n_obs\"] == 0:\n",
    "                                                result[\"n_obs\"] = n_obs\n",
    "                                            \n",
    "                                            # Get min/max from actual data\n",
    "                                            if ens_value == 'mean':\n",
    "                                                result[\"min\"] = float(np.min(valid_data))\n",
    "                                                result[\"max\"] = float(np.max(valid_data))\n",
    "                                                \n",
    "                                                # Get max pixel location\n",
    "                                                max_val = result[\"max\"]\n",
    "                                                indices = np.where((data == max_val) & valid_mask)\n",
    "                                                if len(indices[0]) > 0:\n",
    "                                                    row_idx = indices[0][0]\n",
    "                                                    col_idx = indices[1][0]\n",
    "                                                    max_coord = rasterio.transform.xy(out_transform, row_idx, col_idx)\n",
    "                                                    result[\"max_x\"] = max_coord[0]\n",
    "                                                    result[\"max_y\"] = max_coord[1]\n",
    "                                        \n",
    "                                except Exception as e:\n",
    "                                    pbar_nc.write(f\"⚠ Error extracting {ens_value} for {row['region']}: {e}\")\n",
    "                                finally:\n",
    "                                    memfile.close()\n",
    "                            \n",
    "                            # Add result if we got data\n",
    "                            if result[\"n_obs\"] > 0:\n",
    "                                file_results.append(result)\n",
    "                    \n",
    "                    # Save results for this slice\n",
    "                    if file_results:\n",
    "                        df_file = pd.DataFrame(file_results)\n",
    "                        df_file.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                        pbar_nc.write(f\"  ✓ Saved {len(file_results)} records to {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            pbar_nc.write(f\"⚠ Error processing {os.path.basename(nc_path)}: {e}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar_nc.update(1)\n",
    "    \n",
    "    # Close NC progress bar\n",
    "    pbar_nc.close()\n",
    "    \n",
    "    print(f\"\\n✓ NC processing complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All hazard processing complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f618cc8",
   "metadata": {},
   "source": [
    "## Accessing the Data\n",
    "\n",
    "You can now access the loaded datasets using the hierarchical structure:\n",
    "\n",
    "```python\n",
    "# Access specific dataset\n",
    "ds = return_period_data['Compound']['FWI']['ensemble']['ensemble_return_periods.nc']\n",
    "\n",
    "# Or iterate through all datasets\n",
    "for category in return_period_data:\n",
    "    for hazard in return_period_data[category]:\n",
    "        for model_type in return_period_data[category][hazard]:\n",
    "            for filename, dataset in return_period_data[category][hazard][model_type].items():\n",
    "                # Do something with dataset\n",
    "                pass\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90dc9996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADM boundary shapefiles...\n",
      "  ✓ Loaded ADM1: 27 regions\n",
      "  ✓ Loaded ADM2: 5570 regions\n",
      "\n",
      "============================================================\n",
      "PART 1: Processing TIF flood maps...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RCP8.5 - RP1000: 100%|██████████| 9/9 [00:00<00:00, 58.46file/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭  Already processed: flood_pc_rp10.csv. Skipping.\n",
      "⏭  Already processed: flood_pc_rp100.csv. Skipping.\n",
      "⏭  Already processed: flood_pc_rp1000.csv. Skipping.\n",
      "⚠ Flood map not found: global_rcp26_h10glob.tif. Skipping.\n",
      "⚠ Flood map not found: global_rcp26_h100glob.tif. Skipping.\n",
      "⚠ Flood map not found: global_rcp26_h1000glob.tif. Skipping.\n",
      "⏭  Already processed: flood_rcp85_rp10.csv. Skipping.\n",
      "⏭  Already processed: flood_rcp85_rp100.csv. Skipping.\n",
      "⏭  Already processed: flood_rcp85_rp1000.csv. Skipping.\n",
      "\n",
      "✓ Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- HELPER FUNCTION ---\n",
    "\n",
    "# Note: The fix_text function has been replaced with the more comprehensive\n",
    "# fix_region_encoding function that handles latin1->utf8 corruption reversal\n",
    "# and unidecode processing.\n",
    "\n",
    "\n",
    "# --- LOAD ADM SHAPEFILES ---\n",
    "print(\"Loading ADM boundary shapefiles...\")\n",
    "adm_gdfs = {}\n",
    "\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    # Read the shapefile for this ADM level\n",
    "    try:\n",
    "        gdf = gpd.read_file(adm_path)\n",
    "    except UnicodeDecodeError:\n",
    "        gdf = gpd.read_file(adm_path, encoding='latin1')\n",
    "    \n",
    "    # Create a unique identifier if one doesn't exist\n",
    "    if \"unique_id\" not in gdf.columns:\n",
    "        gdf[\"unique_id\"] = gdf.index + 1\n",
    "\n",
    "    # Determine a region name field by checking common field names\n",
    "    if \"shapeName\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"shapeName\"]\n",
    "    elif \"NAME\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"NAME\"]\n",
    "    elif \"name\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"name\"]\n",
    "    else:\n",
    "        print(f\"Warning: No standard region name field found for {adm_label}. Available fields:\")\n",
    "        print(gdf.columns)\n",
    "        gdf[\"region\"] = gdf[\"unique_id\"]\n",
    "\n",
    "    # Apply comprehensive encoding fixes to region names\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(fix_region_encoding)\n",
    "    \n",
    "    adm_gdfs[adm_label] = gdf\n",
    "    print(f\"  ✓ Loaded {adm_label}: {len(gdf)} regions\")\n",
    "\n",
    "\n",
    "# --- PROCESSING TIF FILES ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 1: Processing TIF flood maps...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate total number of flood maps to process\n",
    "total_flood_maps = len(scenario_codes) * len(return_periods)\n",
    "\n",
    "# Create main progress bar for flood maps\n",
    "pbar_files = tqdm(total=total_flood_maps, desc=\"Processing TIF files\", unit=\"file\", position=0, leave=True, dynamic_ncols=True)\n",
    "\n",
    "# Construct TIF directory path\n",
    "tif_maps_dir = os.path.join(hazards_root_dir, tif_subdir)\n",
    "\n",
    "# Loop over each scenario and return period (i.e., each flood file)\n",
    "for scenario_code in scenario_codes:\n",
    "    scenario_label = scenario_labels[scenario_code]\n",
    "    \n",
    "    for rp in return_periods:\n",
    "        # Construct the flood map path\n",
    "        floodmap_filename = f\"global_{scenario_code}_h{rp}glob.tif\"\n",
    "        floodmap_path = os.path.join(tif_maps_dir, floodmap_filename)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_files.set_description(f\"Processing {scenario_label} - RP{rp}\")\n",
    "        \n",
    "        # Check if output CSV already exists\n",
    "        output_filename = f\"flood_{scenario_code}_rp{rp}.csv\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            pbar_files.write(f\"⏭  Already processed: {output_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "        \n",
    "        # If the flood map file does not exist, skip it\n",
    "        if not os.path.exists(floodmap_path):\n",
    "            pbar_files.write(f\"⚠ Flood map not found: {floodmap_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "\n",
    "        # Results for this specific flood file\n",
    "        file_results = []\n",
    "        \n",
    "        # Open the floodmap raster\n",
    "        with rasterio.open(floodmap_path) as src:\n",
    "            nodata_val = src.nodata\n",
    "            \n",
    "            # Process each ADM level for this flood file\n",
    "            for adm_label, gdf in adm_gdfs.items():\n",
    "                \n",
    "                # Loop through each polygon in the shapefile\n",
    "                for idx, row in gdf.iterrows():\n",
    "                    geom = [row[\"geometry\"]]\n",
    "                    \n",
    "                    try:\n",
    "                        # Mask and crop the raster to the polygon's extent\n",
    "                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                    except Exception as e:\n",
    "                        pbar_files.write(f\"⚠ Error processing region {row['region']} in {adm_label}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract the data from the first band\n",
    "                    data = out_image[0]\n",
    "\n",
    "                    # Create a mask for valid data (exclude nodata values)\n",
    "                    if nodata_val is not None:\n",
    "                        valid_mask = data != nodata_val\n",
    "                        valid_data = data[valid_mask]\n",
    "                    else:\n",
    "                        valid_mask = np.ones(data.shape, dtype=bool)\n",
    "                        valid_data = data\n",
    "\n",
    "                    # If valid_data is a masked array, extract the underlying data\n",
    "                    if np.ma.is_masked(valid_data):\n",
    "                        valid_data = valid_data.compressed()\n",
    "\n",
    "                    # Determine the number of valid observations (pixels)\n",
    "                    n_obs = valid_data.size\n",
    "\n",
    "                    # Compute statistics\n",
    "                    if n_obs == 0:\n",
    "                        stats = {\n",
    "                            \"min\": np.nan,\n",
    "                            \"max\": np.nan,\n",
    "                            \"mean\": np.nan,\n",
    "                            \"median\": np.nan,\n",
    "                            \"p2_5\": np.nan,\n",
    "                            \"p5\": np.nan,\n",
    "                            \"p95\": np.nan,\n",
    "                            \"p97_5\": np.nan\n",
    "                        }\n",
    "                        max_coord = (np.nan, np.nan)\n",
    "                    else:\n",
    "                        stats = {\n",
    "                            \"min\": float(np.min(valid_data)),\n",
    "                            \"max\": float(np.max(valid_data)),\n",
    "                            \"mean\": float(np.mean(valid_data)),\n",
    "                            \"median\": float(np.percentile(valid_data, 50)),\n",
    "                            \"p2_5\": float(np.percentile(valid_data, 2.5)),\n",
    "                            \"p5\": float(np.percentile(valid_data, 5)),\n",
    "                            \"p95\": float(np.percentile(valid_data, 95)),\n",
    "                            \"p97_5\": float(np.percentile(valid_data, 97.5))\n",
    "                        }\n",
    "\n",
    "                        # Identify the pixel location of the maximum value\n",
    "                        max_val = stats[\"max\"]\n",
    "                        indices = np.where((data == max_val) & valid_mask)\n",
    "                        if len(indices[0]) > 0:\n",
    "                            row_idx = indices[0][0]\n",
    "                            col_idx = indices[1][0]\n",
    "                            max_coord = rasterio.transform.xy(out_transform, row_idx, col_idx)\n",
    "                        else:\n",
    "                            max_coord = (np.nan, np.nan)\n",
    "\n",
    "                    # Build the result r cord\n",
    "                    result = {\n",
    "                        \"region\": row[\"region\"],\n",
    "                        \"adm_level\": adm_label,\n",
    "                        \"scenario_code\": scenario_code,\n",
    "                        \"scenario_name\": scenario_label,\n",
    "                        \"hazard_return_period\": rp,\n",
    "                        \"hazard_type\": \"flood\",\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"median\": stats[\"median\"],\n",
    "                        \"p2_5\": stats[\"p2_5\"],\n",
    "                        \"p5\": stats[\"p5\"],\n",
    "                        \"p95\": stats[\"p95\"],\n",
    "                        \"p97_5\": stats[\"p97_5\"],\n",
    "                        \"n_obs\": n_obs,\n",
    "                        \"max_x\": max_coord[0],\n",
    "                        \"max_y\": max_coord[1]\n",
    "                    }\n",
    "\n",
    "                    file_results.append(result)\n",
    "        \n",
    "        # Save results for this file to individual CSV\n",
    "        if file_results:\n",
    "            df_file = pd.DataFrame(file_results)\n",
    "            df_file.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            pbar_files.write(f\"  ✓ Saved {len(file_results)} records to {output_filename}\")\n",
    "        \n",
    "        # Update file progress bar\n",
    "        pbar_files.update(1)\n",
    "\n",
    "# Close main progress bar\n",
    "pbar_files.close()\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065090bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining individual CSV files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV files:   8%|▊         | 2/26 [00:00<00:01, 13.85file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV files: 100%|██████████| 26/26 [00:02<00:00, 12.40file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying encoding fixes to all region names in combined data...\n",
      "✓ Applied encoding fixes to all 7608 unique region names\n",
      "Examples of changes applied:\n",
      "  'Abadia de Goiás' -> 'Abadia de Goias'\n",
      "  'Abadiânia' -> 'Abadiania'\n",
      "  'Abaeté' -> 'Abaete'\n",
      "  'Abaré' -> 'Abare'\n",
      "  'Abatiá' -> 'Abatia'\n",
      "  'Abaíra' -> 'Abaira'\n",
      "  'Abreulândia' -> 'Abreulandia'\n",
      "  'Acaraú' -> 'Acarau'\n",
      "  'Acará' -> 'Acara'\n",
      "  'Acauã' -> 'Acaua'\n",
      "  ... and 2289 more\n",
      "\n",
      "============================================================\n",
      "✓ Combined results saved to: ../../workspace/precomputed_adm_hazards.csv\n",
      "✓ Individual file results saved to: ../../workspace/precomputed_region_results/\n",
      "✓ Total records: 145522\n",
      "✓ CSV files combined: 26\n",
      "✓ Column structure standardized (hazard_return -> hazard_return_period)\n",
      "✓ Hazard return period values preserved from original files\n",
      "✓ Encoding fixes applied universally to all region names\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- COMBINE AND EXPORT ---\n",
    "\n",
    "print(\"\\nCombining individual CSV files...\")\n",
    "\n",
    "# Get all CSV files from the output directory\n",
    "csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "\n",
    "if csv_files:\n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for csv_file in tqdm(csv_files, desc=\"Reading CSV files\", unit=\"file\"):\n",
    "        csv_path = os.path.join(output_dir, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Standardize column names before adding to list\n",
    "        # Some files have 'hazard_return', others have 'hazard_return_period'\n",
    "        # The R codebase expects 'hazard_return_period', so standardize to that\n",
    "        if 'hazard_return' in df.columns and 'hazard_return_period' not in df.columns:\n",
    "            df = df.rename(columns={'hazard_return': 'hazard_return_period'})\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Apply encoding fixes to region names in the combined dataframe\n",
    "    print(\"Applying encoding fixes to all region names in combined data...\")\n",
    "    original_regions = set(df_combined[\"region\"].unique())\n",
    "    df_combined[\"region\"] = df_combined[\"region\"].apply(fix_region_encoding)\n",
    "    fixed_regions = set(df_combined[\"region\"].unique())\n",
    "    \n",
    "    # Report on changes applied\n",
    "    regions_changed = original_regions - fixed_regions\n",
    "    print(f\"✓ Applied encoding fixes to all {len(original_regions)} unique region names\")\n",
    "    if regions_changed:\n",
    "        print(\"Examples of changes applied:\")\n",
    "        for i, region in enumerate(sorted(list(regions_changed))[:10]):\n",
    "            fixed_region = fix_region_encoding(region)\n",
    "            print(f\"  '{region}' -> '{fixed_region}'\")\n",
    "        if len(regions_changed) > 10:\n",
    "            print(f\"  ... and {len(regions_changed) - 10} more\")\n",
    "    else:\n",
    "        print(\"✓ All region names were already properly encoded\")\n",
    "    \n",
    "    # Fill missing values with 0 ONLY for non-hazard_return_period columns\n",
    "    # Preserve original hazard_return_period values\n",
    "    hazard_return_period_values = df_combined['hazard_return_period'].copy()\n",
    "    df_combined = df_combined.fillna(0)\n",
    "    df_combined['hazard_return_period'] = hazard_return_period_values.fillna(0)  # Only fill actual NaN values\n",
    "    \n",
    "    # Save combined results\n",
    "    df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ Combined results saved to: {output_csv}\")\n",
    "    print(f\"✓ Individual file results saved to: {output_dir}/\")\n",
    "    print(f\"✓ Total records: {len(df_combined)}\")\n",
    "    print(f\"✓ CSV files combined: {len(csv_files)}\")\n",
    "    print(f\"✓ Column structure standardized (hazard_return -> hazard_return_period)\")\n",
    "    print(f\"✓ Hazard return period values preserved from original files\")\n",
    "    print(f\"✓ Encoding fixes applied universally to all region names\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n⚠ No CSV files found to combine. Check if any flood maps were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c57111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../workspace/precomputed_region_results'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
