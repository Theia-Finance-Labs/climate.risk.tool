{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce320a-5aab-4852-89c5-b8f43b91d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33502dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INPUTS & OUTPUTS ---\n",
    "##Input here the ADM boundaries ADM1= province, ADM2= Municipality\n",
    "# ADM boundaries\n",
    "adm_levels = [\n",
    "    (\"ADM1\", \"../workspace/boundaries/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp\"),\n",
    "    (\"ADM2\", \"../workspace/boundaries/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp\")\n",
    "]\n",
    "\n",
    "\n",
    "#This script sepeates the flood maps into different categories. We can also leave the full name here aswell, as this might help with the matching later\n",
    "# Flood maps directory and pattern\n",
    "flood_maps_dir = \"../workspace/hazards_world/flood\"\n",
    "\n",
    "# Scenarios and return periods\n",
    "scenario_codes = [\"pc\", \"rcp26\", \"rcp85\"]\n",
    "scenario_labels = {\n",
    "    \"pc\": \"CurrentClimate\",\n",
    "    \"rcp26\": \"RCP2.6\",\n",
    "    \"rcp85\": \"RCP8.5\"\n",
    "}\n",
    "\n",
    "##all the return periods we use, we are currently only using 10 and 100)\n",
    "return_periods = [10, 100, 1000]\n",
    "\n",
    "# Output directory for individual file results\n",
    "output_dir = \"../workspace/precomputed_region_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Final combined output CSV path\n",
    "output_csv = \"../workspace/ADM_allfloods.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HELPER FUNCTION ---\n",
    "\n",
    "def fix_text(s):\n",
    "    \"\"\"\n",
    "    Attempts to fix text encoding (e.g., CP1252 to UTF-8).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.encode('cp1252').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "\n",
    "# --- LOAD ADM SHAPEFILES ---\n",
    "print(\"Loading ADM boundary shapefiles...\")\n",
    "adm_gdfs = {}\n",
    "\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    # Read the shapefile for this ADM level\n",
    "    try:\n",
    "        gdf = gpd.read_file(adm_path)\n",
    "    except UnicodeDecodeError:\n",
    "        gdf = gpd.read_file(adm_path, encoding='latin1')\n",
    "    \n",
    "    # Create a unique identifier if one doesn't exist\n",
    "    if \"unique_id\" not in gdf.columns:\n",
    "        gdf[\"unique_id\"] = gdf.index + 1\n",
    "\n",
    "    # Determine a region name field by checking common field names\n",
    "    if \"shapeName\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"shapeName\"]\n",
    "    elif \"NAME\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"NAME\"]\n",
    "    elif \"name\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"name\"]\n",
    "    else:\n",
    "        print(f\"Warning: No standard region name field found for {adm_label}. Available fields:\")\n",
    "        print(gdf.columns)\n",
    "        gdf[\"region\"] = gdf[\"unique_id\"]\n",
    "\n",
    "    # Fix potential text encoding issues\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(lambda s: fix_text(s) if isinstance(s, str) else s)\n",
    "    \n",
    "    adm_gdfs[adm_label] = gdf\n",
    "    print(f\"  ✓ Loaded {adm_label}: {len(gdf)} regions\")\n",
    "\n",
    "\n",
    "# --- PROCESSING ---\n",
    "print(\"\\nStarting flood map processing...\")\n",
    "\n",
    "# Calculate total number of flood maps to process\n",
    "total_flood_maps = len(scenario_codes) * len(return_periods)\n",
    "\n",
    "# Create main progress bar for flood maps\n",
    "pbar_files = tqdm(total=total_flood_maps, desc=\"Processing flood maps\", unit=\"file\", position=0, leave=True, dynamic_ncols=True)\n",
    "\n",
    "# Loop over each scenario and return period (i.e., each flood file)\n",
    "for scenario_code in scenario_codes:\n",
    "    scenario_label = scenario_labels[scenario_code]\n",
    "    \n",
    "    for rp in return_periods:\n",
    "        # Construct the flood map path\n",
    "        floodmap_filename = f\"global_{scenario_code}_h{rp}glob.tif\"\n",
    "        floodmap_path = os.path.join(flood_maps_dir, floodmap_filename)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_files.set_description(f\"Processing {scenario_label} - RP{rp}\")\n",
    "        \n",
    "        # Check if output CSV already exists\n",
    "        output_filename = f\"flood_{scenario_code}_rp{rp}.csv\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            pbar_files.write(f\"⏭  Already processed: {output_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "        \n",
    "        # If the flood map file does not exist, skip it\n",
    "        if not os.path.exists(floodmap_path):\n",
    "            pbar_files.write(f\"⚠ Flood map not found: {floodmap_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "\n",
    "        # Results for this specific flood file\n",
    "        file_results = []\n",
    "        \n",
    "        # Open the floodmap raster\n",
    "        with rasterio.open(floodmap_path) as src:\n",
    "            nodata_val = src.nodata\n",
    "            \n",
    "            # Process each ADM level for this flood file\n",
    "            for adm_label, gdf in adm_gdfs.items():\n",
    "                \n",
    "                # Loop through each polygon in the shapefile\n",
    "                for idx, row in gdf.iterrows():\n",
    "                    geom = [row[\"geometry\"]]\n",
    "                    \n",
    "                    try:\n",
    "                        # Mask and crop the raster to the polygon's extent\n",
    "                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                    except Exception as e:\n",
    "                        pbar_files.write(f\"⚠ Error processing region {row['region']} in {adm_label}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract the data from the first band\n",
    "                    data = out_image[0]\n",
    "\n",
    "                    # Create a mask for valid data (exclude nodata values)\n",
    "                    if nodata_val is not None:\n",
    "                        valid_mask = data != nodata_val\n",
    "                        valid_data = data[valid_mask]\n",
    "                    else:\n",
    "                        valid_mask = np.ones(data.shape, dtype=bool)\n",
    "                        valid_data = data\n",
    "\n",
    "                    # If valid_data is a masked array, extract the underlying data\n",
    "                    if np.ma.is_masked(valid_data):\n",
    "                        valid_data = valid_data.compressed()\n",
    "\n",
    "                    # Determine the number of valid observations (pixels)\n",
    "                    n_obs = valid_data.size\n",
    "\n",
    "                    # Compute statistics\n",
    "                    if n_obs == 0:\n",
    "                        stats = {\n",
    "                            \"min\": np.nan,\n",
    "                            \"max\": np.nan,\n",
    "                            \"mean\": np.nan,\n",
    "                            \"median\": np.nan,\n",
    "                            \"p2_5\": np.nan,\n",
    "                            \"p5\": np.nan,\n",
    "                            \"p95\": np.nan,\n",
    "                            \"p97_5\": np.nan\n",
    "                        }\n",
    "                        max_coord = (np.nan, np.nan)\n",
    "                    else:\n",
    "                        stats = {\n",
    "                            \"min\": float(np.min(valid_data)),\n",
    "                            \"max\": float(np.max(valid_data)),\n",
    "                            \"mean\": float(np.mean(valid_data)),\n",
    "                            \"median\": float(np.percentile(valid_data, 50)),\n",
    "                            \"p2_5\": float(np.percentile(valid_data, 2.5)),\n",
    "                            \"p5\": float(np.percentile(valid_data, 5)),\n",
    "                            \"p95\": float(np.percentile(valid_data, 95)),\n",
    "                            \"p97_5\": float(np.percentile(valid_data, 97.5))\n",
    "                        }\n",
    "\n",
    "                        # Identify the pixel location of the maximum value\n",
    "                        max_val = stats[\"max\"]\n",
    "                        indices = np.where((data == max_val) & valid_mask)\n",
    "                        if len(indices[0]) > 0:\n",
    "                            row_idx = indices[0][0]\n",
    "                            col_idx = indices[1][0]\n",
    "                            max_coord = rasterio.transform.xy(out_transform, row_idx, col_idx)\n",
    "                        else:\n",
    "                            max_coord = (np.nan, np.nan)\n",
    "\n",
    "                    # Build the result r cord\n",
    "                    result = {\n",
    "                        \"region\": row[\"region\"],\n",
    "                        \"adm_level\": adm_label,\n",
    "                        \"scenario_code\": scenario_code,\n",
    "                        \"scenario_name\": scenario_label,\n",
    "                        \"hazard_return\": rp,\n",
    "                        \"hazard_type\": \"flood\",\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"median\": stats[\"median\"],\n",
    "                        \"p2_5\": stats[\"p2_5\"],\n",
    "                        \"p5\": stats[\"p5\"],\n",
    "                        \"p95\": stats[\"p95\"],\n",
    "                        \"p97_5\": stats[\"p97_5\"],\n",
    "                        \"n_obs\": n_obs,\n",
    "                        \"max_x\": max_coord[0],\n",
    "                        \"max_y\": max_coord[1]\n",
    "                    }\n",
    "\n",
    "                    file_results.append(result)\n",
    "        \n",
    "        # Save results for this file to individual CSV\n",
    "        if file_results:\n",
    "            df_file = pd.DataFrame(file_results)\n",
    "            df_file.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            pbar_files.write(f\"  ✓ Saved {len(file_results)} records to {output_filename}\")\n",
    "        \n",
    "        # Update file progress bar\n",
    "        pbar_files.update(1)\n",
    "\n",
    "# Close main progress bar\n",
    "pbar_files.close()\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065090bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- COMBINE AND EXPORT ---\n",
    "\n",
    "print(\"\\nCombining individual CSV files...\")\n",
    "\n",
    "# Get all CSV files from the output directory\n",
    "csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "\n",
    "if csv_files:\n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for csv_file in tqdm(csv_files, desc=\"Reading CSV files\", unit=\"file\"):\n",
    "        csv_path = os.path.join(output_dir, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    df_combined = df_combined.fillna(0)\n",
    "    \n",
    "    # Save combined results\n",
    "    df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ Combined results saved to: {output_csv}\")\n",
    "    print(f\"✓ Individual file results saved to: {output_dir}/\")\n",
    "    print(f\"✓ Total records: {len(df_combined)}\")\n",
    "    print(f\"✓ CSV files combined: {len(csv_files)}\")\n",
    "    print(f\"✓ Missing values filled with: 0\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n⚠ No CSV files found to combine. Check if any flood maps were processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
