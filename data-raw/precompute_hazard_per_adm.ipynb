{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce320a-5aab-4852-89c5-b8f43b91d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import rasterio.transform\n",
    "from rasterio.io import MemoryFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import netCDF4 as nc\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33502dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INPUTS & OUTPUTS ---\n",
    "##Input here the ADM boundaries ADM1= province, ADM2= Municipality\n",
    "# ADM boundaries\n",
    "adm_levels = [\n",
    "    (\"ADM1\", \"../workspace/boundaries/geoBoundaries-BRA-ADM1-all/geoBoundaries-BRA-ADM1.shp\"),\n",
    "    (\"ADM2\", \"../workspace/boundaries/geoBoundaries-BRA-ADM2-all/geoBoundaries-BRA-ADM2.shp\")\n",
    "]\n",
    "\n",
    "# Hazards root directory (contains both TIF and NC files)\n",
    "hazards_root_dir = \"../workspace/hazards_world\"\n",
    "\n",
    "# TIF-specific configuration\n",
    "tif_subdir = \"flood\"  # Subdirectory within hazards_root_dir for TIF files\n",
    "scenario_codes = [\"pc\", \"rcp26\", \"rcp85\"]\n",
    "scenario_labels = {\n",
    "    \"pc\": \"CurrentClimate\",\n",
    "    \"rcp26\": \"RCP2.6\",\n",
    "    \"rcp85\": \"RCP8.5\"\n",
    "}\n",
    "return_periods = [10, 100, 1000]\n",
    "\n",
    "# NC files will be auto-discovered recursively in hazards_root_dir\n",
    "# Expected structure: hazards/{hazard_type}/{hazard_indicator}/{model_type}/{file}.nc\n",
    "\n",
    "# Output directory for individual file results\n",
    "output_dir = \"../workspace/precomputed_region_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Final combined output CSV path\n",
    "output_csv = \"../workspace/precomputed_adm_hazards.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NC TO RASTER CONVERSION HELPER ---\n",
    "\n",
    "def nc_slice_to_raster(nc_file, gwl_idx, rp_idx, ensemble_value='mean'):\n",
    "    \"\"\"\n",
    "    Convert a NetCDF slice to a rasterio-compatible in-memory raster.\n",
    "    \n",
    "    Uses the same cell-center to cell-edge conversion logic as R code:\n",
    "    - Extract lon/lat coordinates (cell centers)\n",
    "    - Calculate resolution: (max - min) / (n - 1)\n",
    "    - Extend extent by half-pixel: xmin = min(lon) - res/2, xmax = max(lon) + res/2\n",
    "    \n",
    "    Args:\n",
    "        nc_file: Path to NetCDF file\n",
    "        gwl_idx: Index for GWL dimension (0-based)\n",
    "        rp_idx: Index for return_period dimension (0-based)\n",
    "        ensemble_value: Value to filter for ensemble dimension (default: 'mean')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (rasterio dataset in memory, metadata dict)\n",
    "    \"\"\"\n",
    "    dataset = nc.Dataset(nc_file, 'r')\n",
    "    \n",
    "    # Find the main data variable (prefer one ending with '_max')\n",
    "    var_names = [v for v in dataset.variables.keys() \n",
    "                 if v not in ['lat', 'lon', 'latitude', 'longitude', 'GWL', 'gwl', 'return_period', 'ensemble']]\n",
    "    main_var = None\n",
    "    for v in var_names:\n",
    "        if v.endswith('_max'):\n",
    "            main_var = v\n",
    "            break\n",
    "    if main_var is None and var_names:\n",
    "        main_var = var_names[0]\n",
    "    \n",
    "    if main_var is None:\n",
    "        dataset.close()\n",
    "        return None, None\n",
    "    \n",
    "    var = dataset.variables[main_var]\n",
    "    dim_names = [d for d in var.dimensions]\n",
    "    \n",
    "    # Identify dimension names (case-insensitive)\n",
    "    lon_dim = next((d for d in dim_names if d.lower() in ['lon', 'longitude', 'x']), 'lon')\n",
    "    lat_dim = next((d for d in dim_names if d.lower() in ['lat', 'latitude', 'y']), 'lat')\n",
    "    ens_dim = next((d for d in dim_names if d.lower() == 'ensemble'), None)\n",
    "    gwl_dim = next((d for d in dim_names if d.lower() in ['gwl', 'GWL']), None)\n",
    "    \n",
    "    # Remaining dimension is likely return_period\n",
    "    rp_dim = next((d for d in dim_names \n",
    "                   if d not in [lon_dim, lat_dim, ens_dim, gwl_dim]), 'return_period')\n",
    "    \n",
    "    # Get coordinate values\n",
    "    lon_vals = dataset.variables[lon_dim][:]\n",
    "    lat_vals = dataset.variables[lat_dim][:]\n",
    "    \n",
    "    # Find ensemble index\n",
    "    ens_idx = 0\n",
    "    if ens_dim and ens_dim in dataset.variables:\n",
    "        ens_vals = dataset.variables[ens_dim][:]\n",
    "        if ens_vals.dtype.kind in ['U', 'S']:  # String type\n",
    "            ens_list = [str(e) if isinstance(e, bytes) else e for e in ens_vals]\n",
    "            if ensemble_value in ens_list:\n",
    "                ens_idx = ens_list.index(ensemble_value)\n",
    "    \n",
    "    # Build slice indices\n",
    "    slice_dict = {}\n",
    "    for d in dim_names:\n",
    "        if d == lon_dim or d == lat_dim:\n",
    "            slice_dict[d] = slice(None)\n",
    "        elif d == ens_dim:\n",
    "            slice_dict[d] = ens_idx\n",
    "        elif d == gwl_dim:\n",
    "            slice_dict[d] = gwl_idx\n",
    "        elif d == rp_dim:\n",
    "            slice_dict[d] = rp_idx\n",
    "        else:\n",
    "            slice_dict[d] = 0\n",
    "    \n",
    "    # Extract 2D slice\n",
    "    # Build proper indexing tuple based on dimension order\n",
    "    idx_tuple = tuple(slice_dict[d] for d in dim_names)\n",
    "    data_slice = var[idx_tuple]\n",
    "    \n",
    "    # Ensure 2D\n",
    "    if data_slice.ndim > 2:\n",
    "        data_slice = np.squeeze(data_slice)\n",
    "    \n",
    "    # Calculate resolution and extent (cell-center to cell-edge conversion)\n",
    "    n_lon = len(lon_vals)\n",
    "    n_lat = len(lat_vals)\n",
    "    \n",
    "    res_lon = (np.max(lon_vals) - np.min(lon_vals)) / (n_lon - 1) if n_lon > 1 else 1.0\n",
    "    res_lat = (np.max(lat_vals) - np.min(lat_vals)) / (n_lat - 1) if n_lat > 1 else 1.0\n",
    "    \n",
    "    # Extend by half-pixel\n",
    "    xmin = np.min(lon_vals) - res_lon / 2\n",
    "    xmax = np.max(lon_vals) + res_lon / 2\n",
    "    ymin = np.min(lat_vals) - res_lat / 2\n",
    "    ymax = np.max(lat_vals) + res_lat / 2\n",
    "    \n",
    "    # Ensure correct orientation: if data is (lon, lat), transpose to (lat, lon)\n",
    "    if data_slice.shape[0] == n_lon and data_slice.shape[1] == n_lat:\n",
    "        data_slice = data_slice.T\n",
    "    \n",
    "    # Flip vertically so first row is max(lat)\n",
    "    data_slice = np.flipud(data_slice)\n",
    "    \n",
    "    # Create transform\n",
    "    transform = rasterio.transform.from_bounds(xmin, ymin, xmax, ymax, n_lon, n_lat)\n",
    "    \n",
    "    # Create in-memory raster\n",
    "    memfile = MemoryFile()\n",
    "    with memfile.open(\n",
    "        driver='GTiff',\n",
    "        height=n_lat,\n",
    "        width=n_lon,\n",
    "        count=1,\n",
    "        dtype=data_slice.dtype,\n",
    "        crs='EPSG:4326',\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(data_slice, 1)\n",
    "    \n",
    "    # Get metadata for inventory\n",
    "    gwl_vals = dataset.variables[gwl_dim][:] if gwl_dim and gwl_dim in dataset.variables else [gwl_idx]\n",
    "    rp_vals = dataset.variables[rp_dim][:] if rp_dim in dataset.variables else [rp_idx]\n",
    "    \n",
    "    gwl_label = str(gwl_vals[gwl_idx]) if gwl_idx < len(gwl_vals) else f\"idx{gwl_idx}\"\n",
    "    rp_label = str(rp_vals[rp_idx]) if rp_idx < len(rp_vals) else f\"idx{rp_idx}\"\n",
    "    \n",
    "    metadata = {\n",
    "        'gwl': gwl_label,\n",
    "        'return_period': rp_label,\n",
    "        'main_var': main_var\n",
    "    }\n",
    "    \n",
    "    dataset.close()\n",
    "    \n",
    "    # Return the memfile (stays open) and metadata\n",
    "    return memfile, metadata\n",
    "\n",
    "\n",
    "def discover_nc_files(hazards_dir):\n",
    "    \"\"\"\n",
    "    Discover all NC files and extract their metadata from path structure.\n",
    "    Expected: hazards/{hazard_type}/{hazard_indicator}/{model_type}/{file}.nc\n",
    "    \n",
    "    Returns:\n",
    "        list of dicts with file path and parsed metadata\n",
    "    \"\"\"\n",
    "    nc_files = glob.glob(os.path.join(hazards_dir, '**', '*.nc'), recursive=True)\n",
    "    \n",
    "    results = []\n",
    "    for f in nc_files:\n",
    "        # Parse path components\n",
    "        parts = os.path.normpath(f).split(os.sep)\n",
    "        \n",
    "        if len(parts) >= 4:\n",
    "            file_name = parts[-1]\n",
    "            model_type = parts[-2]\n",
    "            hazard_indicator = parts[-3]\n",
    "            hazard_type = parts[-4]\n",
    "        else:\n",
    "            # Fallback\n",
    "            file_name = os.path.basename(f)\n",
    "            model_type = 'ensemble'\n",
    "            hazard_indicator = 'indicator'\n",
    "            hazard_type = 'unknown'\n",
    "        \n",
    "        results.append({\n",
    "            'path': f,\n",
    "            'hazard_type': hazard_type,\n",
    "            'hazard_indicator': hazard_indicator,\n",
    "            'model_type': model_type,\n",
    "            'file_name': file_name\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6373b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PROCESSING NC FILES ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 2: Processing NetCDF files...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Discover all NC files\n",
    "nc_files_info = discover_nc_files(hazards_root_dir)\n",
    "\n",
    "if not nc_files_info:\n",
    "    print(\"⚠ No NetCDF files found. Skipping NC processing.\")\n",
    "else:\n",
    "    print(f\"Found {len(nc_files_info)} NetCDF file(s)\")\n",
    "    \n",
    "    # Create progress bar for NC files\n",
    "    pbar_nc = tqdm(total=len(nc_files_info), desc=\"Processing NC files\", unit=\"file\", position=0, leave=True, dynamic_ncols=True)\n",
    "    \n",
    "    for nc_info in nc_files_info:\n",
    "        nc_path = nc_info['path']\n",
    "        hazard_type = nc_info['hazard_type']\n",
    "        hazard_indicator = nc_info['hazard_indicator']\n",
    "        \n",
    "        pbar_nc.set_description(f\"Processing {hazard_type}/{hazard_indicator}\")\n",
    "        \n",
    "        # Open NC file to discover dimensions\n",
    "        try:\n",
    "            dataset = nc.Dataset(nc_path, 'r')\n",
    "            \n",
    "            # Find dimensions\n",
    "            dim_names = list(dataset.dimensions.keys())\n",
    "            gwl_dim = next((d for d in dim_names if d.lower() in ['gwl', 'GWL']), None)\n",
    "            rp_dim = next((d for d in dim_names if d.lower() in ['return_period']), None)\n",
    "            \n",
    "            # Get dimension sizes\n",
    "            n_gwl = len(dataset.dimensions[gwl_dim]) if gwl_dim else 1\n",
    "            n_rp = len(dataset.dimensions[rp_dim]) if rp_dim else 1\n",
    "            \n",
    "            # Get actual values for labeling\n",
    "            gwl_vals = dataset.variables[gwl_dim][:] if gwl_dim and gwl_dim in dataset.variables else list(range(n_gwl))\n",
    "            rp_vals = dataset.variables[rp_dim][:] if rp_dim and rp_dim in dataset.variables else list(range(n_rp))\n",
    "            \n",
    "            dataset.close()\n",
    "            \n",
    "            # Process each GWL x return_period combination\n",
    "            for ig in range(n_gwl):\n",
    "                for ir in range(n_rp):\n",
    "                    gwl_label = str(gwl_vals[ig]) if ig < len(gwl_vals) else f\"idx{ig}\"\n",
    "                    rp_label = str(rp_vals[ir]) if ir < len(rp_vals) else f\"idx{ir}\"\n",
    "                    \n",
    "                    # Check if output already exists\n",
    "                    output_filename = f\"{hazard_type}_{hazard_indicator}_gwl{gwl_label}_rp{rp_label}.csv\"\n",
    "                    output_path = os.path.join(output_dir, output_filename)\n",
    "                    \n",
    "                    if os.path.exists(output_path):\n",
    "                        pbar_nc.write(f\"⏭  Already processed: {output_filename}. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert NC slice to raster\n",
    "                    memfile, metadata = nc_slice_to_raster(nc_path, ig, ir, ensemble_value='mean')\n",
    "                    \n",
    "                    if memfile is None:\n",
    "                        pbar_nc.write(f\"⚠ Failed to process slice GWL={gwl_label}, RP={rp_label} from {os.path.basename(nc_path)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Results for this slice\n",
    "                    file_results = []\n",
    "                    \n",
    "                    # Open the in-memory raster\n",
    "                    with memfile.open() as src:\n",
    "                        nodata_val = src.nodata\n",
    "                        \n",
    "                        # Process each ADM level\n",
    "                        for adm_label, gdf in adm_gdfs.items():\n",
    "                            \n",
    "                            # Loop through each polygon\n",
    "                            for idx, row in gdf.iterrows():\n",
    "                                geom = [row[\"geometry\"]]\n",
    "                                \n",
    "                                try:\n",
    "                                    # Mask and crop the raster\n",
    "                                    out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                                except Exception as e:\n",
    "                                    pbar_nc.write(f\"⚠ Error processing region {row['region']} in {adm_label}: {e}\")\n",
    "                                    continue\n",
    "                                \n",
    "                                # Extract the data from the first band\n",
    "                                data = out_image[0]\n",
    "                                \n",
    "                                # Create a mask for valid data\n",
    "                                if nodata_val is not None:\n",
    "                                    valid_mask = data != nodata_val\n",
    "                                    valid_data = data[valid_mask]\n",
    "                                else:\n",
    "                                    valid_mask = np.ones(data.shape, dtype=bool)\n",
    "                                    valid_data = data\n",
    "                                \n",
    "                                # Handle masked arrays\n",
    "                                if np.ma.is_masked(valid_data):\n",
    "                                    valid_data = valid_data.compressed()\n",
    "                                \n",
    "                                n_obs = valid_data.size\n",
    "                                \n",
    "                                # Compute statistics\n",
    "                                if n_obs == 0:\n",
    "                                    stats = {\n",
    "                                        \"min\": np.nan,\n",
    "                                        \"max\": np.nan,\n",
    "                                        \"mean\": np.nan,\n",
    "                                        \"median\": np.nan,\n",
    "                                        \"p2_5\": np.nan,\n",
    "                                        \"p5\": np.nan,\n",
    "                                        \"p95\": np.nan,\n",
    "                                        \"p97_5\": np.nan\n",
    "                                    }\n",
    "                                    max_coord = (np.nan, np.nan)\n",
    "                                else:\n",
    "                                    stats = {\n",
    "                                        \"min\": float(np.min(valid_data)),\n",
    "                                        \"max\": float(np.max(valid_data)),\n",
    "                                        \"mean\": float(np.mean(valid_data)),\n",
    "                                        \"median\": float(np.percentile(valid_data, 50)),\n",
    "                                        \"p2_5\": float(np.percentile(valid_data, 2.5)),\n",
    "                                        \"p5\": float(np.percentile(valid_data, 5)),\n",
    "                                        \"p95\": float(np.percentile(valid_data, 95)),\n",
    "                                        \"p97_5\": float(np.percentile(valid_data, 97.5))\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Identify max pixel location\n",
    "                                    max_val = stats[\"max\"]\n",
    "                                    indices = np.where((data == max_val) & valid_mask)\n",
    "                                    if len(indices[0]) > 0:\n",
    "                                        row_idx = indices[0][0]\n",
    "                                        col_idx = indices[1][0]\n",
    "                                        max_coord = rasterio.transform.xy(out_transform, row_idx, col_idx)\n",
    "                                    else:\n",
    "                                        max_coord = (np.nan, np.nan)\n",
    "                                \n",
    "                                # Build result record\n",
    "                                result = {\n",
    "                                    \"region\": row[\"region\"],\n",
    "                                    \"adm_level\": adm_label,\n",
    "                                    \"scenario_code\": gwl_label,\n",
    "                                    \"scenario_name\": gwl_label,\n",
    "                                    \"hazard_return_period\": rp_label,\n",
    "                                    \"hazard_type\": hazard_type,\n",
    "                                    \"hazard_indicator\": hazard_indicator,\n",
    "                                    \"min\": stats[\"min\"],\n",
    "                                    \"max\": stats[\"max\"],\n",
    "                                    \"mean\": stats[\"mean\"],\n",
    "                                    \"median\": stats[\"median\"],\n",
    "                                    \"p2_5\": stats[\"p2_5\"],\n",
    "                                    \"p5\": stats[\"p5\"],\n",
    "                                    \"p95\": stats[\"p95\"],\n",
    "                                    \"p97_5\": stats[\"p97_5\"],\n",
    "                                    \"n_obs\": n_obs,\n",
    "                                    \"max_x\": max_coord[0],\n",
    "                                    \"max_y\": max_coord[1]\n",
    "                                }\n",
    "                                \n",
    "                                file_results.append(result)\n",
    "                    \n",
    "                    # Save results for this slice\n",
    "                    if file_results:\n",
    "                        df_file = pd.DataFrame(file_results)\n",
    "                        df_file.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                        pbar_nc.write(f\"  ✓ Saved {len(file_results)} records to {output_filename}\")\n",
    "                    \n",
    "                    # Close the memfile\n",
    "                    memfile.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            pbar_nc.write(f\"⚠ Error processing {os.path.basename(nc_path)}: {e}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar_nc.update(1)\n",
    "    \n",
    "    # Close NC progress bar\n",
    "    pbar_nc.close()\n",
    "    \n",
    "    print(f\"\\n✓ NC processing complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All hazard processing complete!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HELPER FUNCTION ---\n",
    "\n",
    "def fix_text(s):\n",
    "    \"\"\"\n",
    "    Attempts to fix text encoding (e.g., CP1252 to UTF-8).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return s.encode('cp1252').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "\n",
    "# --- LOAD ADM SHAPEFILES ---\n",
    "print(\"Loading ADM boundary shapefiles...\")\n",
    "adm_gdfs = {}\n",
    "\n",
    "for adm_label, adm_path in adm_levels:\n",
    "    # Read the shapefile for this ADM level\n",
    "    try:\n",
    "        gdf = gpd.read_file(adm_path)\n",
    "    except UnicodeDecodeError:\n",
    "        gdf = gpd.read_file(adm_path, encoding='latin1')\n",
    "    \n",
    "    # Create a unique identifier if one doesn't exist\n",
    "    if \"unique_id\" not in gdf.columns:\n",
    "        gdf[\"unique_id\"] = gdf.index + 1\n",
    "\n",
    "    # Determine a region name field by checking common field names\n",
    "    if \"shapeName\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"shapeName\"]\n",
    "    elif \"NAME\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"NAME\"]\n",
    "    elif \"name\" in gdf.columns:\n",
    "        gdf[\"region\"] = gdf[\"name\"]\n",
    "    else:\n",
    "        print(f\"Warning: No standard region name field found for {adm_label}. Available fields:\")\n",
    "        print(gdf.columns)\n",
    "        gdf[\"region\"] = gdf[\"unique_id\"]\n",
    "\n",
    "    # Fix potential text encoding issues\n",
    "    gdf[\"region\"] = gdf[\"region\"].apply(lambda s: fix_text(s) if isinstance(s, str) else s)\n",
    "    \n",
    "    adm_gdfs[adm_label] = gdf\n",
    "    print(f\"  ✓ Loaded {adm_label}: {len(gdf)} regions\")\n",
    "\n",
    "\n",
    "# --- PROCESSING TIF FILES ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 1: Processing TIF flood maps...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate total number of flood maps to process\n",
    "total_flood_maps = len(scenario_codes) * len(return_periods)\n",
    "\n",
    "# Create main progress bar for flood maps\n",
    "pbar_files = tqdm(total=total_flood_maps, desc=\"Processing TIF files\", unit=\"file\", position=0, leave=True, dynamic_ncols=True)\n",
    "\n",
    "# Construct TIF directory path\n",
    "tif_maps_dir = os.path.join(hazards_root_dir, tif_subdir)\n",
    "\n",
    "# Loop over each scenario and return period (i.e., each flood file)\n",
    "for scenario_code in scenario_codes:\n",
    "    scenario_label = scenario_labels[scenario_code]\n",
    "    \n",
    "    for rp in return_periods:\n",
    "        # Construct the flood map path\n",
    "        floodmap_filename = f\"global_{scenario_code}_h{rp}glob.tif\"\n",
    "        floodmap_path = os.path.join(tif_maps_dir, floodmap_filename)\n",
    "        \n",
    "        # Update progress bar description\n",
    "        pbar_files.set_description(f\"Processing {scenario_label} - RP{rp}\")\n",
    "        \n",
    "        # Check if output CSV already exists\n",
    "        output_filename = f\"flood_{scenario_code}_rp{rp}.csv\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            pbar_files.write(f\"⏭  Already processed: {output_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "        \n",
    "        # If the flood map file does not exist, skip it\n",
    "        if not os.path.exists(floodmap_path):\n",
    "            pbar_files.write(f\"⚠ Flood map not found: {floodmap_filename}. Skipping.\")\n",
    "            pbar_files.update(1)\n",
    "            continue\n",
    "\n",
    "        # Results for this specific flood file\n",
    "        file_results = []\n",
    "        \n",
    "        # Open the floodmap raster\n",
    "        with rasterio.open(floodmap_path) as src:\n",
    "            nodata_val = src.nodata\n",
    "            \n",
    "            # Process each ADM level for this flood file\n",
    "            for adm_label, gdf in adm_gdfs.items():\n",
    "                \n",
    "                # Loop through each polygon in the shapefile\n",
    "                for idx, row in gdf.iterrows():\n",
    "                    geom = [row[\"geometry\"]]\n",
    "                    \n",
    "                    try:\n",
    "                        # Mask and crop the raster to the polygon's extent\n",
    "                        out_image, out_transform = rasterio.mask.mask(src, geom, crop=True)\n",
    "                    except Exception as e:\n",
    "                        pbar_files.write(f\"⚠ Error processing region {row['region']} in {adm_label}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    # Extract the data from the first band\n",
    "                    data = out_image[0]\n",
    "\n",
    "                    # Create a mask for valid data (exclude nodata values)\n",
    "                    if nodata_val is not None:\n",
    "                        valid_mask = data != nodata_val\n",
    "                        valid_data = data[valid_mask]\n",
    "                    else:\n",
    "                        valid_mask = np.ones(data.shape, dtype=bool)\n",
    "                        valid_data = data\n",
    "\n",
    "                    # If valid_data is a masked array, extract the underlying data\n",
    "                    if np.ma.is_masked(valid_data):\n",
    "                        valid_data = valid_data.compressed()\n",
    "\n",
    "                    # Determine the number of valid observations (pixels)\n",
    "                    n_obs = valid_data.size\n",
    "\n",
    "                    # Compute statistics\n",
    "                    if n_obs == 0:\n",
    "                        stats = {\n",
    "                            \"min\": np.nan,\n",
    "                            \"max\": np.nan,\n",
    "                            \"mean\": np.nan,\n",
    "                            \"median\": np.nan,\n",
    "                            \"p2_5\": np.nan,\n",
    "                            \"p5\": np.nan,\n",
    "                            \"p95\": np.nan,\n",
    "                            \"p97_5\": np.nan\n",
    "                        }\n",
    "                        max_coord = (np.nan, np.nan)\n",
    "                    else:\n",
    "                        stats = {\n",
    "                            \"min\": float(np.min(valid_data)),\n",
    "                            \"max\": float(np.max(valid_data)),\n",
    "                            \"mean\": float(np.mean(valid_data)),\n",
    "                            \"median\": float(np.percentile(valid_data, 50)),\n",
    "                            \"p2_5\": float(np.percentile(valid_data, 2.5)),\n",
    "                            \"p5\": float(np.percentile(valid_data, 5)),\n",
    "                            \"p95\": float(np.percentile(valid_data, 95)),\n",
    "                            \"p97_5\": float(np.percentile(valid_data, 97.5))\n",
    "                        }\n",
    "\n",
    "                        # Identify the pixel location of the maximum value\n",
    "                        max_val = stats[\"max\"]\n",
    "                        indices = np.where((data == max_val) & valid_mask)\n",
    "                        if len(indices[0]) > 0:\n",
    "                            row_idx = indices[0][0]\n",
    "                            col_idx = indices[1][0]\n",
    "                            max_coord = rasterio.transform.xy(out_transform, row_idx, col_idx)\n",
    "                        else:\n",
    "                            max_coord = (np.nan, np.nan)\n",
    "\n",
    "                    # Build the result r cord\n",
    "                    result = {\n",
    "                        \"region\": row[\"region\"],\n",
    "                        \"adm_level\": adm_label,\n",
    "                        \"scenario_code\": scenario_code,\n",
    "                        \"scenario_name\": scenario_label,\n",
    "                        \"hazard_return_period\": rp,\n",
    "                        \"hazard_type\": \"flood\",\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"median\": stats[\"median\"],\n",
    "                        \"p2_5\": stats[\"p2_5\"],\n",
    "                        \"p5\": stats[\"p5\"],\n",
    "                        \"p95\": stats[\"p95\"],\n",
    "                        \"p97_5\": stats[\"p97_5\"],\n",
    "                        \"n_obs\": n_obs,\n",
    "                        \"max_x\": max_coord[0],\n",
    "                        \"max_y\": max_coord[1]\n",
    "                    }\n",
    "\n",
    "                    file_results.append(result)\n",
    "        \n",
    "        # Save results for this file to individual CSV\n",
    "        if file_results:\n",
    "            df_file = pd.DataFrame(file_results)\n",
    "            df_file.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            pbar_files.write(f\"  ✓ Saved {len(file_results)} records to {output_filename}\")\n",
    "        \n",
    "        # Update file progress bar\n",
    "        pbar_files.update(1)\n",
    "\n",
    "# Close main progress bar\n",
    "pbar_files.close()\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065090bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- COMBINE AND EXPORT ---\n",
    "\n",
    "print(\"\\nCombining individual CSV files...\")\n",
    "\n",
    "# Get all CSV files from the output directory\n",
    "csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "\n",
    "if csv_files:\n",
    "    # Read and combine all CSV files\n",
    "    dfs = []\n",
    "    for csv_file in tqdm(csv_files, desc=\"Reading CSV files\", unit=\"file\"):\n",
    "        csv_path = os.path.join(output_dir, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes\n",
    "    df_combined = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    df_combined = df_combined.fillna(0)\n",
    "    \n",
    "    # Save combined results\n",
    "    df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ Combined results saved to: {output_csv}\")\n",
    "    print(f\"✓ Individual file results saved to: {output_dir}/\")\n",
    "    print(f\"✓ Total records: {len(df_combined)}\")\n",
    "    print(f\"✓ CSV files combined: {len(csv_files)}\")\n",
    "    print(f\"✓ Missing values filled with: 0\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n⚠ No CSV files found to combine. Check if any flood maps were processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
